# Testing Improvements - Next Steps Roadmap

**Date**: 2025-01-22
**Context**: Post testing anti-pattern elimination (169 patterns eliminated, 46‚Üí0 warnings)
**Related**: [refactor-action-plan.md](./refactor-action-plan.md) Phase 5, [error-handling.md](./error-handling.md) Phase 0
**Maintenu par**: Claude Code + Julien Barbe

---

## üéØ OBJECTIF

Renforcer la couverture et la qualit√© des tests suite √† l'√©limination des anti-patterns en :
1. Ajoutant des tests d'int√©gration avec DuckDB r√©elle
2. Impl√©mentant l'analyse de couverture de code
3. Optimisant les performances des tests

---

## üìä √âTAT ACTUEL

### ‚úÖ Travail Compl√©t√© (2025-01-22)

**8 fichiers de tests corrig√©s** avec √©limination des anti-patterns :
- `tests/cli/test_stats.py` - FakeRegistry ‚Üí Real EntityRegistry (40 lignes de mock ‚Üí production code)
- `tests/cli/test_transform.py` - ResourceWarning fix via Database mocking
- `tests/core/services/test_transformer.py` - Documentation des limitations (unit vs integration)
- `tests/core/services/test_exporter.py` - Documentation orchestration tests
- `tests/core/services/test_importer.py` - Ajout spec= aux mocks Database/Registry
- `tests/core/plugins/transformers/aggregation/test_field_aggregator.py` - Mocks au bon niveau (db.fetch_one vs private methods)
- `tests/common/test_database.py` - Documentation mocking infrastructure (commit/rollback errors)
- `tests/common/test_environment.py` - Filesystem r√©el au lieu de mocks

**R√©sultats quantifi√©s** :
- **169 anti-patterns √©limin√©s** :
  - Testing Mock Behavior ‚Üí Testing Real Behavior
  - Mocking Private Methods ‚Üí Mocking External Dependencies
  - Incomplete Mocks ‚Üí Mocks with spec=
  - Hybrid DB/Mock Tests ‚Üí Documented Infrastructure Mocking
- **3 bugs critiques trouv√©s** pendant le refactoring :
  - Environment reset incomplet (fichiers non supprim√©s)
  - Bug de conversion de type (string "500" au lieu de int 500)
  - **S√âCURIT√â** : Database supprim√©e sans confirmation (ajout --force-reset flag)
- **Warnings r√©duits** : 46 ‚Üí 0
  - ResourceWarnings (connexions non ferm√©es) : 19 ‚Üí 0
  - UserWarnings (ipywidgets, pandas, geopandas) : 22 ‚Üí 0
  - DeprecationWarnings (pyproj, shapely) : 5 ‚Üí 0
- **201+ tests am√©lior√©s** suivent maintenant les best practices

### üìã M√©triques de Qualit√© des Tests

| M√©trique | Avant | Apr√®s | Cible |
|----------|-------|-------|-------|
| Anti-patterns | 169 | 0 ‚úÖ | 0 |
| ResourceWarnings | 46 | 0 ‚úÖ | 0 |
| Tests avec documentation | ~20% | ~60% | 100% |
| Tests d'int√©gration | 11 | 11 | 50+ |
| Couverture de code | ~80% | ~80% | >90% |
| Temps d'ex√©cution | ~18s | ~17s | <10s |

### ‚ö†Ô∏è Limitations Actuelles

**Tests principalement unitaires** :
- `test_transformer.py:19` - NOTE explicite : "Ces tests ne v√©rifient PAS les r√©sultats r√©els des transformations"
- `test_exporter.py:10` - LIMITATION : "Ces tests ne v√©rifient PAS la cr√©ation r√©elle de fichiers"
- `test_stats.py` - Base de donn√©es en m√©moire mais ex√©cution SQL mock√©e

**Gaps de couverture** :
- Transformers g√©ospatiaux : ~75% (cas d'erreur manquants)
- GUI code : <40% (largement non test√©)
- Cas edge : Configurations invalides, erreurs DB non test√©es

**Performance** :
- Suite compl√®te : ~17s (acceptable mais peut √™tre am√©lior√©)
- Fixtures DB recr√©√©es plusieurs fois (optimisation possible)

---

## üìà RECOMMENDED NEXT STEPS

### STEP 1 - Tests d'Int√©gration avec DuckDB R√©elle üî¥

**Dur√©e Estim√©e** : 2-3 jours
**Priorit√©** : **HAUTE** - Critique pour valider le comportement r√©el
**Effort** : üî¥üî¥üî¥ Moyen-√âlev√©

#### Pourquoi C'est N√©cessaire

Les tests actuels sont principalement des tests unitaires avec d√©pendances mock√©es :
- `test_transformer.py` mocke les transformers ET la database (ligne 19 NOTE explique la limitation)
- `test_stats.py` utilise une base en m√©moire mais mocke l'ex√©cution SQL
- `test_exporter.py` mocke les exporters et ne v√©rifie pas les fichiers cr√©√©s
- **Aucun test end-to-end** du pipeline complet : Import CSV ‚Üí Transform ‚Üí Export

**Impact sur la qualit√©** :
- Bugs de production non d√©tect√©s (comme le bug de type string‚Üíint d√©couvert)
- Comportement r√©el des plugins non valid√©
- Int√©gration DuckDB/SQLAlchemy non test√©e
- Sc√©narios complexes (hi√©rarchies, g√©ospatial) non couverts

#### Approche Propos√©e

**1. Cr√©er structure `tests/integration/`** :

```
tests/integration/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ conftest.py                    # Fixtures communes
‚îú‚îÄ‚îÄ test_import_transform_flow.py  # Pipeline complet
‚îú‚îÄ‚îÄ test_real_transformers.py      # Transformers avec vraie DB
‚îú‚îÄ‚îÄ test_hierarchy_transformers.py # adjacency_list, nested_set
‚îú‚îÄ‚îÄ test_geospatial_transformers.py # raster_stats, vector_overlay
‚îî‚îÄ‚îÄ fixtures/
    ‚îú‚îÄ‚îÄ sample_occurrences.csv
    ‚îú‚îÄ‚îÄ sample_taxonomy.csv
    ‚îî‚îÄ‚îÄ sample_shapes.geojson
```

**2. Fixture pour DuckDB r√©elle** :

```python
# tests/integration/conftest.py
import pytest
from pathlib import Path
from niamoto.common.database import Database
from niamoto.core.imports.registry import EntityRegistry

@pytest.fixture(scope="module")
def integration_db(tmp_path_factory):
    """Create real DuckDB with test data for integration tests.

    Scope: module - DB cr√©√©e une fois par fichier de test pour performance.
    """
    db_path = tmp_path_factory.mktemp("integration") / "test.duckdb"
    db = Database(str(db_path))
    registry = EntityRegistry(db)

    # Load test taxonomy
    registry.register_entity(
        name="taxon_ref",
        kind=EntityKind.REFERENCE,
        table_name="taxon_ref",
        config={}
    )

    # Import sample data
    # ... (CSV loading)

    yield db, registry

    # Cleanup
    db.engine.dispose()
    db_path.unlink(missing_ok=True)


@pytest.fixture
def sample_occurrences_csv(tmp_path):
    """Create sample occurrences CSV for testing."""
    csv_path = tmp_path / "occurrences.csv"
    df = pd.DataFrame({
        "id": [1, 2, 3],
        "taxon_ref_id": [100, 101, 100],
        "plot_ref_id": [1, 1, 2],
        "dbh": [25.5, 42.0, 18.3],
        "elevation": [450, 450, 680]
    })
    df.to_csv(csv_path, index=False)
    return csv_path
```

**3. Tests d'int√©gration end-to-end** :

```python
# tests/integration/test_import_transform_flow.py
"""Integration tests for complete Import ‚Üí Transform ‚Üí Export flow.

Unlike unit tests that mock components, these tests:
- Use real DuckDB database with test data
- Load real transformer plugins
- Verify actual transformed data in database
- Test complete pipeline without mocks
"""

def test_complete_pipeline_taxon_aggregation(integration_db, sample_occurrences_csv):
    """Test: Import occurrences ‚Üí Transform by taxon ‚Üí Verify results in DB."""
    db, registry = integration_db

    # 1. IMPORT: Load occurrences from CSV
    importer = ImporterService(db.db_path)
    config = DatasetEntityConfig(
        connector=ConnectorConfig(type=ConnectorType.FILE, path=str(sample_occurrences_csv)),
        schema=EntitySchema(id_field="id", fields=[])
    )
    result = importer.import_dataset("occurrences", config)
    assert "Imported 3 records" in result

    # 2. TRANSFORM: Aggregate by taxon
    transformer = TransformerService(db.db_path, mock_config)
    transformer.transform_data(group_by="taxon", recreate_table=True)

    # 3. VERIFY: Check transformed data in database
    result = db.execute_sql(
        "SELECT taxon_id, occurrence_count FROM taxon WHERE taxon_id = 100",
        fetch=True
    )
    assert result["occurrence_count"] == 2  # Real DB query, real result

    # 4. VERIFY: Check widget results stored
    result = db.execute_sql(
        "SELECT widget_data FROM taxon WHERE taxon_id = 100",
        fetch=True
    )
    widget_data = json.loads(result["widget_data"])
    assert "species_count" in widget_data
```

**4. Tests de transformers sp√©cifiques** :

```python
# tests/integration/test_real_transformers.py

def test_field_aggregator_with_real_db(integration_db):
    """Test FieldAggregator retrieves values from real database."""
    db, registry = integration_db

    # Setup: Insert test data
    db.execute_sql("""
        INSERT INTO entity_plots (id, plot_name, elevation)
        VALUES (1, 'Plot A', 450), (2, 'Plot B', 680)
    """)

    # Test: Aggregate field from database source
    aggregator = FieldAggregator(db, registry)
    config = FieldAggregatorParams(
        field="elevation",
        sources={"elevation": {"entity": "plots", "field": "elevation"}},
        operation="direct"
    )

    data = pd.DataFrame({"plot_ref_id": [1, 2]})
    result = aggregator.transform(data, config)

    # Verify: Real values from database
    assert result["elevation"][0] == 450  # From real DB, not mock
    assert result["elevation"][1] == 680
```

#### Sc√©narios de Test √† Couvrir

**Import ‚Üí Transform flows** :
- [ ] Occurrences ‚Üí Agr√©gation par taxon (count, avg DBH)
- [ ] Occurrences ‚Üí Agr√©gation par plot (species richness)
- [ ] Multi-source aggregation (taxon + shapes + occurrences)
- [ ] Custom entities (pas seulement taxon_ref/plot_ref par d√©faut)

**Hierarchies** :
- [ ] adjacency_list transformer avec vraie taxonomie
- [ ] nested_set transformer avec vraie taxonomie
- [ ] Hi√©rarchie complexe (>3 niveaux)

**Geospatial** :
- [ ] raster_stats avec vraies donn√©es raster (GeoTIFF)
- [ ] vector_overlay avec vrais shapefiles
- [ ] Reprojection CRS (EPSG:4326 ‚Üí UTM)
- [ ] Clip, intersection, union operations

**Error scenarios** :
- [ ] Fichier CSV corrompu
- [ ] Configuration invalide (champs manquants)
- [ ] √âchec de transformation (division par z√©ro)
- [ ] Contraintes DB viol√©es (duplicate IDs)

#### Livrables

**Code** :
- [ ] `tests/integration/` cr√©√© avec structure compl√®te
- [ ] `conftest.py` avec fixtures `integration_db`, `sample_*_csv`
- [ ] 20+ tests d'int√©gration couvrant :
  - [ ] 5+ tests Import ‚Üí Transform ‚Üí Verify flow
  - [ ] 5+ tests transformers sp√©cifiques avec vraie DB
  - [ ] 3+ tests hi√©rarchies (adjacency_list, nested_set)
  - [ ] 4+ tests g√©ospatiaux (raster, vector, CRS)
  - [ ] 3+ tests sc√©narios d'erreur

**Documentation** :
- [ ] Docstring module expliquant diff√©rence unit vs integration
- [ ] README `tests/integration/README.md` expliquant :
  - Quand √©crire un test d'int√©gration vs unitaire
  - Comment lancer uniquement les tests d'int√©gration
  - Comment cr√©er des fixtures de test data
- [ ] Mise √† jour `pytest.ini` avec marker `integration`

**CI/CD** :
- [ ] S√©paration tests unitaires (rapides) vs int√©gration (lents)
- [ ] Pipeline CI lance unit tests sur chaque commit
- [ ] Pipeline CI lance integration tests sur PR vers main

---

### STEP 2 - Analyse de Couverture de Code üü°

**Dur√©e Estim√©e** : 1 jour
**Priorit√©** : **MOYENNE** - Important pour identifier les gaps
**Effort** : üü°üü° Moyen

#### Gaps de Couverture Actuels

**D'apr√®s analyse `coverage.json`** :
- **Transformers** : ~75% (cas d'erreur manquants, edge cases)
- **Services** : ~85% (m√©thodes priv√©es non test√©es)
- **CLI commands** : ~70% (flags/options non test√©s)
- **Utils** : ~60% (helpers peu test√©s)
- **GUI** : <40% (API endpoints largement non test√©s)

**Zones probl√©matiques identifi√©es** :
- `core/plugins/transformers/geospatial/raster_stats.py:150-180` - Gestion erreurs raster
- `core/plugins/transformers/ecological/elevation_profile.py` - Edge cases altitudes
- `core/services/exporter.py:200-250` - Logique de rollback non test√©e
- `gui/api/routes/imports.py` - Endpoints file upload non test√©s

#### Approche Propos√©e

**1. Setup coverage.py avec reporting HTML** :

```toml
# pyproject.toml - Ajouter
[tool.coverage.run]
source = ["src/niamoto"]
omit = [
    "*/tests/*",
    "*/migrations/*",
    "*/__pycache__/*",
    "*/site-packages/*",
]
branch = true  # Branch coverage (if/else paths)

[tool.coverage.report]
precision = 2
show_missing = true
skip_covered = false

[tool.coverage.html]
directory = "htmlcov"

[tool.pytest.ini_options]
addopts = """
    --cov=src/niamoto
    --cov-report=html
    --cov-report=term-missing:skip-covered
    --cov-report=json
"""
```

**2. G√©n√©rer rapport de couverture initial** :

```bash
# Baseline actuel
uv run pytest --cov=src/niamoto --cov-report=html --cov-report=term-missing

# Ouvrir rapport HTML
open htmlcov/index.html

# Identifier fichiers <90%
grep -E "\"percent_covered\": [0-8][0-9]\." coverage.json
```

**3. Cibler zones prioritaires** :

```python
# Exemple: Ajouter tests pour raster_stats edge cases
# tests/core/plugins/transformers/geospatial/test_raster_stats_coverage.py

def test_raster_stats_invalid_raster_path():
    """Test handling of non-existent raster file."""
    plugin = RasterStatsPlugin(db, registry)
    config = RasterStatsParams(
        raster_path="/nonexistent/raster.tif",
        stats=["mean"]
    )

    with pytest.raises(FileNotFoundError, match="Raster file not found"):
        plugin.transform(sample_gdf, config)


def test_raster_stats_corrupted_raster():
    """Test handling of corrupted GeoTIFF."""
    # Create corrupted raster file
    corrupted_path = tmp_path / "corrupted.tif"
    corrupted_path.write_bytes(b"not a valid GeoTIFF")

    plugin = RasterStatsPlugin(db, registry)
    config = RasterStatsParams(
        raster_path=str(corrupted_path),
        stats=["mean"]
    )

    with pytest.raises(RasterIOError, match="Failed to read raster"):
        plugin.transform(sample_gdf, config)


def test_raster_stats_crs_mismatch_auto_reproject():
    """Test automatic reprojection when CRS mismatch."""
    # GeoDataFrame in EPSG:4326, raster in EPSG:32758
    gdf = create_sample_gdf(crs="EPSG:4326")

    plugin = RasterStatsPlugin(db, registry)
    config = RasterStatsParams(
        raster_path=str(utm_raster_path),  # EPSG:32758
        stats=["mean"],
        auto_reproject=True
    )

    result = plugin.transform(gdf, config)

    # Verify reprojection happened and stats calculated
    assert "raster_mean" in result.columns
    assert not result["raster_mean"].isna().all()
```

**4. Zones cibles par module** :

| Module | Couverture Actuelle | Cible | Tests √† Ajouter |
|--------|---------------------|-------|-----------------|
| **Core Plugins** | ~75% | >90% | +50 tests (edge cases, errors) |
| **Services** | ~85% | >90% | +20 tests (error paths, rollback) |
| **CLI Commands** | ~70% | >85% | +30 tests (flags, help, errors) |
| **Utils** | ~60% | >80% | +25 tests (helpers, validators) |
| **GUI API** | <40% | >75% | +40 tests (endpoints, file upload) |

#### Livrables

**Setup** :
- [ ] Coverage.py configur√© dans `pyproject.toml`
- [ ] CI g√©n√®re rapport de couverture sur chaque PR
- [ ] Badge coverage dans README.md

**Rapports** :
- [ ] Rapport HTML baseline g√©n√©r√© (`htmlcov/`)
- [ ] Liste fichiers <90% avec plan d'am√©lioration
- [ ] Tracking couverture dans GitHub Actions

**Tests** :
- [ ] +50 tests pour core plugins (edge cases, errors)
- [ ] +20 tests pour services (error handling, rollback)
- [ ] +30 tests pour CLI commands (flags, options)
- [ ] +40 tests pour GUI API (endpoints, validation)

**Documentation** :
- [ ] Guide "Comment am√©liorer la couverture" dans `tests/README.md`
- [ ] Pre-commit hook v√©rifie couverture >85% sur fichiers modifi√©s

---

### STEP 3 - Optimisation des Performances üü¢

**Dur√©e Estim√©e** : 1-2 jours
**Priorit√©** : **BASSE** - Nice to have, pas bloquant
**Effort** : üü¢ Faible-Moyen

#### Issues de Performance Actuels

**Temps d'ex√©cution mesur√©s** :
- Suite compl√®te : ~17s (acceptable mais am√©liorable)
- Tests unitaires : ~12s
- Tests d'int√©gration : ~5s (va augmenter avec Step 1)
- Slowest tests :
  - `test_stats.py::test_stats_command_full_flow` : ~0.8s
  - `test_exporter.py::test_export_all` : ~0.6s
  - `test_transformer.py::test_full_transformation_workflow` : ~0.5s

**Causes identifi√©es** :
- Fixtures DB recr√©√©es plusieurs fois (scope="function" au lieu de "module")
- Plugin loading √† chaque test (peut √™tre cach√©)
- Tests s√©quentiels (pas de parall√©lisation)
- In-memory DB pas utilis√©e pour unit tests (fichiers temporaires)

#### Approche Propos√©e

**1. Benchmark performance baseline** :

```python
# tests/performance/test_suite_benchmark.py
"""Benchmark suite to track test performance over time.

Run with: pytest tests/performance/ --benchmark-only
"""
import pytest
import time
from datetime import datetime

@pytest.fixture
def performance_log(tmp_path):
    """Log performance metrics to JSON for tracking."""
    log_file = tmp_path / "performance_log.json"
    return log_file


def test_unit_tests_performance_baseline(benchmark):
    """Baseline: Unit tests should run in <15s."""
    def run_unit_tests():
        # Run pytest programmatically
        pytest.main(["-m", "not integration", "--tb=no"])

    result = benchmark(run_unit_tests)
    assert result < 15.0, f"Unit tests took {result}s, target <15s"


def test_integration_tests_performance_baseline(benchmark):
    """Baseline: Integration tests should run in <2min."""
    def run_integration_tests():
        pytest.main(["-m", "integration", "--tb=no"])

    result = benchmark(run_integration_tests)
    assert result < 120.0, f"Integration tests took {result}s, target <120s"
```

**2. Optimisations √† impl√©menter** :

**A. Fixture scoping optimal** :

```python
# tests/conftest.py - AVANT (lent)
@pytest.fixture
def test_db(tmp_path):
    """Database recreated for EACH test."""
    db_path = tmp_path / "test.db"
    db = Database(str(db_path))
    yield db
    db.engine.dispose()

# tests/conftest.py - APR√àS (rapide)
@pytest.fixture(scope="module")
def test_db(tmp_path_factory):
    """Database created ONCE per test module."""
    db_path = tmp_path_factory.mktemp("data") / "test.db"
    db = Database(str(db_path))
    yield db
    db.engine.dispose()
    db_path.unlink(missing_ok=True)
```

**B. In-memory DuckDB pour tests unitaires** :

```python
# tests/conftest.py
@pytest.fixture(scope="session")
def inmemory_db():
    """In-memory DuckDB for fast unit tests.

    Shared across ALL tests in session for maximum speed.
    """
    db = Database(":memory:")  # In-memory, pas de I/O disque
    yield db
    db.engine.dispose()
```

**C. Plugin loading cach√©** :

```python
# tests/conftest.py
@pytest.fixture(scope="session")
def plugin_loader_cached():
    """Cache plugin loading for entire test session."""
    loader = PluginLoader()
    loader.load_core_plugins()
    # Plugins charg√©s UNE FOIS pour toute la session
    return loader
```

**D. Parall√©lisation avec pytest-xdist** :

```bash
# Ajouter √† pyproject.toml
[project.optional-dependencies]
dev = [
    # ... existing deps
    "pytest-xdist>=3.5.0",  # Parallel test execution
]

# Lancer tests en parall√®le (auto-detect CPU cores)
uv run pytest -n auto

# Ou sp√©cifier nombre de workers
uv run pytest -n 4
```

**E. Markers pour tests lents** :

```python
# pytest.ini
[tool.pytest.ini_options]
markers = [
    "integration: Integration tests (slow)",
    "slow: Slow tests (>1s)",
]

# Dans tests
@pytest.mark.slow
def test_large_dataset_processing():
    """Test with 10k+ records - slow."""
    # ...

# Skip slow tests en d√©veloppement
pytest -m "not slow"
```

**3. M√©triques cibles** :

| Suite | Actuel | Cible | Optimisation |
|-------|--------|-------|-------------|
| **Unit tests** | ~12s | <8s | Fixture scoping, in-memory DB |
| **Integration tests** | ~5s | <30s | M√™me apr√®s ajout de 20+ tests (Step 1) |
| **Full suite (sequential)** | ~17s | <40s | Apr√®s ajout Step 1 tests |
| **Full suite (parallel -n 4)** | N/A | <15s | pytest-xdist |
| **CI pipeline** | ~1min | <2min | Parallel + caching |

#### Livrables

**Benchmarking** :
- [ ] Suite de benchmarks dans `tests/performance/`
- [ ] Baseline actuel document√©
- [ ] CI track performance regression (fail si >10% plus lent)

**Optimisations** :
- [ ] Fixtures avec scope optimal (session/module/function)
- [ ] In-memory DB pour unit tests
- [ ] Plugin loading cach√© (scope="session")
- [ ] pytest-xdist configur√© pour parall√©lisation

**Documentation** :
- [ ] Guide "Running tests efficiently" dans `tests/README.md`
  - Lancer uniquement unit tests rapides
  - Skip slow tests en d√©veloppement
  - Parall√©liser avec -n auto
- [ ] Pre-commit hook lance uniquement tests rapides (<5s)

**CI/CD** :
- [ ] GitHub Actions utilise pytest-xdist (-n 4)
- [ ] Cache pip/uv dependencies
- [ ] Tests s√©par√©s : unit (rapide) ‚Üí integration (si unit pass)

---

## üìã CHECKLIST D√âTAILL√â

### Week 1: Integration Tests (Step 1)

**Lundi** :
- [ ] Cr√©er structure `tests/integration/`
- [ ] Impl√©menter fixture `integration_db` avec vraie DuckDB
- [ ] Cr√©er fixtures sample data (CSV, GeoJSON)
- [ ] √âcrire 5 premiers tests : Import ‚Üí Transform ‚Üí Verify

**Mardi** :
- [ ] Ajouter 5 tests custom entities (pas taxon/plot par d√©faut)
- [ ] Ajouter 3 tests hi√©rarchies (adjacency_list, nested_set)
- [ ] Tester multi-source aggregations

**Mercredi** :
- [ ] Ajouter 4 tests g√©ospatiaux (raster_stats, vector_overlay)
- [ ] Tester reprojections CRS automatiques
- [ ] Tester clip, intersection, union operations

**Jeudi** :
- [ ] Ajouter 3+ tests sc√©narios d'erreur (CSV corrompu, config invalide)
- [ ] Documenter tests d'int√©gration (README, docstrings)
- [ ] Configurer marker `integration` dans pytest.ini

**Vendredi** :
- [ ] Review tests avec √©quipe
- [ ] Refactoring si n√©cessaire
- [ ] CI/CD: S√©parer unit vs integration tests
- [ ] **Milestone** : 20+ integration tests fonctionnels ‚úÖ

---

### Week 2: Coverage + Performance (Steps 2 & 3)

**Lundi** :
- [ ] Setup coverage.py dans `pyproject.toml`
- [ ] G√©n√©rer rapport baseline (`htmlcov/`)
- [ ] Identifier fichiers <90% avec gaps sp√©cifiques
- [ ] Cr√©er issues GitHub pour chaque gap majeur

**Mardi** :
- [ ] √âcrire +25 tests pour core plugins (edge cases)
- [ ] √âcrire +10 tests pour services (error handling)
- [ ] √âcrire +15 tests pour CLI commands (flags)
- [ ] Target: Core plugins >90%

**Mercredi** :
- [ ] √âcrire +20 tests pour GUI API endpoints
- [ ] Configurer CI g√©n√®re rapport coverage sur PR
- [ ] Ajouter badge coverage dans README.md
- [ ] Target: Services >90%, CLI >85%

**Jeudi** :
- [ ] Performance benchmarking baseline
- [ ] Optimiser fixtures (scope="module/session")
- [ ] Impl√©menter in-memory DB pour unit tests
- [ ] Configurer pytest-xdist pour parall√©lisation

**Vendredi** :
- [ ] Tester performance apr√®s optimisations
- [ ] Documenter "Running tests efficiently"
- [ ] Configurer CI utilise pytest-xdist
- [ ] Final review & validation
- [ ] **Milestone** : Coverage >90%, Tests <15s en parall√®le ‚úÖ

---

## üéØ CRIT√àRES DE SUCC√àS

### Must-Have (Bloquants pour Phase 5 du refactor)

‚úÖ **Integration Tests** :
- [ ] Au minimum 20 tests d'int√©gration avec vraie DuckDB
- [ ] Coverage du pipeline complet : Import ‚Üí Transform ‚Üí Export
- [ ] Tests g√©ospatiaux et hi√©rarchies couverts
- [ ] CI s√©pare unit tests (rapides) vs integration (lents)

‚úÖ **Code Coverage** :
- [ ] Core plugins : >90% coverage
- [ ] Services : >90% coverage
- [ ] CLI commands : >85% coverage
- [ ] CI √©choue si coverage r√©gresse >5%

‚úÖ **Quality Gates** :
- [ ] Tous les tests passent (0 failures, 0 warnings)
- [ ] Documentation √† jour (README, docstrings)
- [ ] Aucun anti-pattern r√©introduit

---

### Should-Have (Hautement recommand√©)

üü° **Performance** :
- [ ] Benchmarks de performance √©tablis
- [ ] Suite compl√®te <40s (s√©quentiel) ou <15s (parall√®le -n 4)
- [ ] Fixtures optimis√©es (scope appropri√©)

üü° **GUI Coverage** :
- [ ] API endpoints >75% coverage
- [ ] File upload endpoint test√©
- [ ] Validation errors test√©es

üü° **Documentation** :
- [ ] Guide "Integration vs Unit tests"
- [ ] Guide "Running tests efficiently"
- [ ] Pre-commit hooks configur√©s

---

### Nice-to-Have (Bonus)

üü¢ **CI/CD Advanced** :
- [ ] Parallel test execution automatique en CI
- [ ] Test performance dashboard (track over time)
- [ ] Automated coverage regression detection (fail si <85%)

üü¢ **Developer Experience** :
- [ ] VS Code test runner configur√©
- [ ] Test templates/snippets pour nouveaux tests
- [ ] Live coverage feedback pendant d√©veloppement

üü¢ **Advanced Testing** :
- [ ] Property-based testing avec Hypothesis (existant dans deps)
- [ ] Mutation testing (v√©rifie qualit√© des tests)
- [ ] Contract testing pour API GUI

---

## üìö R√âF√âRENCES

### Documents Li√©s

- **[Refactor Action Plan](./refactor-action-plan.md)** - Phase 5 (Tests & Validation) - Ce roadmap pr√©pare cette phase
- **[Error Handling Roadmap](./error-handling.md)** - Phase 0 (Validation Framework) - Les tests d'int√©gration valident error handling
- **[Refactor Status](./refactor-status-2025-01-20.md)** - Progression actuelle (Phase 1 Plugins 56%) - Tests qualit√© critique pour migration

### Fichiers Modifi√©s lors de l'√âlimination des Anti-Patterns (2025-01-22)

**Tests corrig√©s** :
- `tests/cli/test_stats.py` - FakeRegistry ‚Üí EntityRegistry (40 lines mock ‚Üí production)
- `tests/cli/test_transform.py` - ResourceWarning fix via Database mock
- `tests/core/services/test_transformer.py` - Documentation limitations (line 7-18)
- `tests/core/services/test_exporter.py` - Documentation orchestration tests (line 7-13)
- `tests/core/services/test_importer.py` - spec= ajout√© aux mocks (line 31, 43, 52)
- `tests/core/plugins/transformers/aggregation/test_field_aggregator.py` - Mock correct level
- `tests/common/test_database.py` - Infrastructure mocking justifi√© (line 438-454)
- `tests/common/test_environment.py` - Real filesystem vs mocks

**Config** :
- `pyproject.toml` - filterwarnings ajout√©s (line 268-285)

### Code Examples

**Anti-pattern AVANT** (Testing Mock Behavior) :
```python
# tests/core/plugins/transformers/aggregation/test_field_aggregator.py (OLD)
def test_transform_with_db_source(self, mocker):
    # ‚ùå Mock la m√©thode priv√©e au lieu de la d√©pendance externe
    mock_get_field = mocker.patch.object(
        self.plugin, "_get_field_value", return_value=500
    )
    result = self.plugin.transform(SAMPLE_DATA.copy(), config)
    assert result == expected_result
    mock_get_field.assert_called_once()  # ‚ùå Test le mock, pas le comportement
```

**Best practice APR√àS** (Testing Real Behavior) :
```python
# tests/core/plugins/transformers/aggregation/test_field_aggregator.py (NEW)
def test_transform_with_db_source(self, mocker):
    """‚úÖ Mock database au bon niveau (d√©pendance externe)."""
    mocker.patch.object(
        self.plugin.registry,
        "get",
        return_value=SimpleNamespace(table_name="entity_plots")
    )
    # ‚úÖ Mock l'external dependency (database fetch)
    mocker.patch.object(
        self.db_mock,
        "fetch_one",
        return_value={"plot_value": 500}
    )

    result = self.plugin.transform(SAMPLE_DATA.copy(), config)

    # ‚úÖ Test le r√©sultat r√©el, pas les appels mock
    assert result == {"db_direct_value": {"value": "500"}}

    # Bug r√©v√©l√©: valeur retourn√©e comme string "500" au lieu de int 500!
```

### Testing Best Practices Appliqu√©es

1. **Mock the external dependency, not the internal logic**
   - ‚úÖ Mock `db.fetch_one()` (external)
   - ‚ùå Ne pas mock `_get_field_value()` (internal/private)

2. **Use spec= parameter to catch invalid method calls**
   - ‚úÖ `Mock(spec=Database)` d√©tecte appels invalides
   - ‚ùå `Mock()` accepte silencieusement tout

3. **Test real behavior with temporary files (tmp_path)**
   - ‚úÖ `test_environment.py` utilise `tmp_path` pour vraie suppression fichiers
   - ‚ùå Ne pas mock `os.remove()`, `shutil.rmtree()`

4. **Document test limitations clearly**
   - ‚úÖ `test_transformer.py:7-18` - NOTE explicite sur limitations
   - ‚úÖ `test_exporter.py:7-13` - LIMITATION document√©e

5. **Verify parameters and outcomes, not just that functions were called**
   - ‚úÖ `assert result["value"] == expected_value`
   - ‚ùå `mock.assert_called_once()` seul (insuffisant)

---

## üîÑ PROCHAINES √âTAPES IMM√âDIATES

**Aujourd'hui (J+0)** :
- [ ] Cr√©er branche `feature/integration-tests`
- [ ] Cr√©er structure `tests/integration/`
- [ ] Setup fixtures de base (`conftest.py`)

**Cette semaine** :
- [ ] Impl√©menter Step 1 (Integration Tests)
- [ ] Daily standup : Review progress on checklist

**Semaine prochaine** :
- [ ] Impl√©menter Steps 2 & 3 (Coverage + Performance)
- [ ] Final review avant merge

**Review Points** :
- [ ] Apr√®s Week 1 : Review integration tests avec √©quipe
- [ ] Apr√®s Week 2 : Review coverage + performance metrics
- [ ] Avant merge : Validation tous crit√®res Must-Have remplis

---

**Derni√®re mise √† jour** : 2025-01-22
**Prochaine review** : Apr√®s Step 1 completion (fin Week 1)
**Status** : üü° EN COURS - Step 1 to start
