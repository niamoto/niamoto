# Generic Import System - Analyse & Plan de Refactorisation

**Date**: 2025-10-07
**Auteur**: Claude Code
**Statut**: Analyse approfondie
**Version**: Alpha - Pas de contrainte de r√©trocompatibilit√©

---

## Table des Mati√®res

0. [Journal d'avancement](#journal-davancement)
1. [Executive Summary](#1-executive-summary)
2. [Analyse de l'Architecture Actuelle](#2-analyse-de-larchitecture-actuelle)
3. [Probl√®mes Identifi√©s & Impact](#3-probl√®mes-identifi√©s--impact)
4. [Vision Cible](#4-vision-cible)
   - 4.1 [Principes Directeurs](#41-principes-directeurs)
   - 4.2 [Entity Registry Architecture](#42-entity-registry-architecture)
   - 4.3 [Import Planner & Connectors](#43-import-planner--connectors)
   - 4.4 [Lifecycle & Observability](#44-lifecycle--observability)
   - 4.5 [Configuration Cible](#45-configuration-cible)
   - 4.6 [Sch√©ma de Base de Donn√©es Cible (DuckDB)](#46-sch√©ma-de-base-de-donn√©es-cible-duckdb)
   - 4.7 [Configuration Transform Cible](#47-configuration-transform-cible)
5. [Analyse des Risques & Contraintes](#5-analyse-des-risques--contraintes)
6. [Solution Propos√©e](#6-solution-propos√©e)
7. [Roadmap D√©taill√©e](#7-roadmap-d√©taill√©e)
8. [Points de D√©cision Critiques](#8-points-de-d√©cision-critiques)
9. [M√©triques de Succ√®s](#9-m√©triques-de-succ√®s)
10. [Conclusion & Next Steps](#conclusion--next-steps)
11. Annexe : Impact DuckDB

---

## Journal d'avancement

### 2025-10-08 (Matin)
- ‚úÖ GeospatialExtractor, loader `direct_reference`, services Transformer/Exporter
  et endpoints GUI `/table-fields` & `/status` consomment d√©sormais la registry et
  les helpers DuckDB.
- ‚úÖ `legacy_registry` et `direct_reference_legacy` servent d'amorce pour les
  tables historiques tout en garantissant la transition vers DuckDB.
- ‚úÖ Loader `join_table` et les endpoints GUI `/schema`, `/tables/*`, `query` s'appuient d√©sormais sur la registry/DuckDB (plus de requ√™tes directes sur `sqlite_master`).

### 2025-10-08 (Apr√®s-midi) - Migration Compl√®te Generic Import System
- ‚úÖ **Suppression compl√®te du code legacy**:
  - √âlimin√© `Config.get_imports_config` legacy - retourne uniquement `GenericImportConfig`
  - Supprim√© compl√®tement `src/niamoto/core/components/imports/`
  - Supprim√© tous les mod√®les SQLAlchemy legacy (TaxonRef, PlotRef, OccurrenceModel, ShapeRef)

- ‚úÖ **Refactorisation compl√®te de l'ImporterService**:
  - Nouvelles m√©thodes: `import_reference()`, `import_dataset()`, `import_all()`
  - Tables g√©n√©riques: `entity_{name}` pour r√©f√©rences, `dataset_{name}` pour datasets
  - Support complet de `ReferenceEntityConfig` et `DatasetEntityConfig`

- ‚úÖ **Refactorisation CLI**:
  - Nouvelles commandes: `niamoto import run`, `niamoto import reference <name>`, `niamoto import dataset <name>`, `niamoto import list`
  - Suppression des commandes legacy (`taxonomy`, `occurrences`, `plots`, `shapes`)

- ‚úÖ **Refactorisation API GUI Backend**:
  - Endpoints g√©n√©riques: `/api/imports/execute/all`, `/api/imports/execute/reference/{entity_name}`, `/api/imports/execute/dataset/{entity_name}`
  - Endpoints de m√©tadonn√©es: `/api/imports/entities`, `/api/imports/status`, `/api/imports/jobs`
  - Suppression de tous les endpoints hardcod√©s (taxonomy, occurrences, plots, shapes)

- ‚úÖ **Mise √† jour des hooks TypeScript**:
  - `useImportStatus.ts` utilise maintenant `ImportStatusResponse` avec `references[]` et `datasets[]`
  - `import.ts` refactoris√© avec `executeImport()`, `executeImportAll()`, `getEntities()`

- ‚úÖ **Ajout de `ReferenceKind` √† config_models.py**:
  - Enum pour `hierarchical`, `spatial`, `categorical`, `generic`

- ‚úÖ **Tests compl√®tement r√©√©crits**:
  - `tests/core/services/test_importer.py` - 7 tests passent (100%)
  - Tests couvrent: import reference, import dataset, import_all, reset_table, validation

- üìå **Note Frontend UI**: Les composants React utilisent encore l'ancienne structure hardcod√©e.
  Une refonte compl√®te de l'UI sera n√©cessaire dans une phase ult√©rieure.

### 2025-10-09 - R√©f√©rences D√©riv√©es et Fix Critique

- ‚úÖ **Syst√®me de r√©f√©rences d√©riv√©es op√©rationnel**:
  - HierarchyBuilder avec extraction DuckDB CTEs niveau par niveau
  - Support `connector.type: derived` pour extraction automatique depuis datasets
  - IDs stables g√©n√©r√©s via hash MD5 des paths hi√©rarchiques

- ‚úÖ **Fix critique `incomplete_rows: skip`**:
  - **Probl√®me**: Filtrage global trop strict (183 taxons au lieu de 1667)
  - **Solution**: Filtrage niveau par niveau dans `_build_extraction_cte()` (lignes 138-142)
  - **R√©sultat**: 1667 taxons extraits correctement (95 familles, 297 genres, 1213 esp√®ces, 62 infra)

- ‚úÖ **Migration instance test `niamoto-nc`**:
  - DuckDB op√©rationnel (13 MB)
  - 203 865 occurrences import√©es
  - Registry persist√© avec 3 entit√©s (occurrences, taxonomy, plots)
  - Configuration v2 en production

- ‚úÖ **Tests**:
  - `test_importer.py`: 7/7 tests passent
  - `test_hierarchy_builder.py`: 4/5 tests passent (1 test obsol√®te √† mettre √† jour)

### 2025-10-10 - D√©marrage Phase 8 : Plugins G√©n√©riques

- üöß **Phase 8 EN COURS**: Refactorisation des 12 plugins pour √©liminer hardcoded table names
- üöß **Objectif**: Supprimer `legacy_registry.py` et tous les alias
- üöß **Audit plugins**: Documentation des modifications requises (voir `phase8-plugin-audit.md`)

## Actions restantes (Octobre 2025)

- ‚úÖ Supprimer `src/niamoto/core/components/imports` et les mod√®les SQLAlchemy fixes - **TERMIN√â**
- ‚úÖ Fix `incomplete_rows: skip` pour extraction hi√©rarchies partielles - **TERMIN√â**
- ‚úÖ Instance test `niamoto-nc` migr√©e et op√©rationnelle - **TERMIN√â**
- üöß **Phase 8 EN COURS**: Refactoriser 12 plugins pour accepter EntityRegistry et √©liminer hardcoded table names
- üöß Cr√©er widget GUI `entity-select` pour s√©lection dynamique d'entit√©s
- üöß Supprimer `legacy_registry.py` une fois plugins g√©n√©riques
- üöß Consolider les tests d'int√©gration : seed registry partiel, `niamoto stats`, endpoints GUI `/schema`/`/query` sous DuckDB
- üöß Documenter la configuration g√©n√©rique c√¥t√© GUI/CLI (guides + exemples) et mettre √† jour les snapshots transform/export apr√®s retrait legacy

---

## 1. Executive Summary

### Contexte
Niamoto poss√®de actuellement :
- ‚úÖ **Transform** : Syst√®me g√©n√©rique bas√© sur plugins
- ‚úÖ **Export** : Syst√®me g√©n√©rique bas√© sur plugins
- ‚ùå **Import** : Syst√®me rigide avec tables et mod√®les fixes

**Statut Alpha** : Pas de contrainte de r√©trocompatibilit√©, possibilit√© de breaking changes.

### Objectif
Rendre l'import aussi g√©n√©rique que transform/export, permettant aux utilisateurs de :
- D√©finir leurs propres entit√©s de r√©f√©rence (pas seulement taxonomy/plots/shapes)
- Importer n'importe quelle structure de donn√©es
- Bootstrapper automatiquement une instance depuis des fichiers bruts

### Enjeux Critiques
1. ~~**R√©trocompatibilit√©**~~ : ‚ùå N/A - Version alpha
2. **Coh√©rence Import-Transform-Export** : Nommage et structure doivent √™tre unifi√©s
3. **Performance** : Sch√©ma dynamique ne doit pas d√©grader les performances
4. **Complexit√©** : Ne pas rendre le syst√®me trop abstrait au point d'√™tre incompr√©hensible

### Recommandation
**Refactoring direct en 3 phases** - 8 semaines :
1. **Phase 1** (3 semaines) : Core abstractions + Generic import engine
2. **Phase 2** (3 semaines) : Bootstrap & auto-detection
3. **Phase 3** (2 semaines) : GUI integration

---

## 2. Analyse de l'Architecture Actuelle

### 2.1 Import System (Rigide)

#### Structure des Donn√©es
```yaml
# import.yml - Structure actuelle
taxonomy:
  path: imports/occurrences.csv
  hierarchy:
    levels: [family, genus, species, infra]
    taxon_id_column: id_taxonref

plots:
  type: csv
  path: imports/plots.csv
  identifier: id_plot
  locality_field: plot
  location_field: geo_pt

occurrences:
  type: csv
  path: imports/occurrences.csv
  identifier: id_taxonref
  location_field: geo_pt
```

#### Mod√®les de Donn√©es (SQLAlchemy)
```python
class TaxonRef(Base):
    __tablename__ = "taxon_ref"
    id = Column(Integer, primary_key=True)
    taxon_id = Column(Integer)
    full_name = Column(String(255))
    rank_name = Column(String(50))
    lft, rght, level = Columns(Integer)  # Nested Set
    parent_id = Column(Integer, ForeignKey("taxon_ref.id"))
    extra_data = Column(JSON)

class PlotRef(Base):
    __tablename__ = "plot_ref"
    # Structure similaire avec nested set

class ShapeRef(Base):
    __tablename__ = "shape_ref"
    # Structure similaire avec nested set
```

#### Importers Sp√©cialis√©s
- `TaxonomyImporter` : 906 lignes, g√®re hierarchies taxonomiques
- `PlotImporter` : 1579 lignes, g√®re geometries et hierarchies spatiales
- `OccurrenceImporter` : 679 lignes, g√®re liens avec taxonomie
- `ShapeImporter` : Non lu mais similaire

#### Service d'Import
```python
class ImporterService:
    def __init__(self, db_path: str):
        self.taxonomy_importer = TaxonomyImporter(self.db)
        self.occurrence_importer = OccurrenceImporter(self.db)
        self.plot_importer = PlotImporter(self.db)
        self.shape_importer = ShapeImporter(self.db)
```

**Forces :**
- ‚úÖ Robuste et test√©
- ‚úÖ Validation sp√©cifique √† chaque type
- ‚úÖ Gestion des cas particuliers (nested sets, geometries)
- ‚úÖ Performance optimis√©e

**Faiblesses :**
- ‚ùå Tables et noms hardcod√©s
- ‚ùå Impossible d'ajouter de nouveaux types d'entit√©s
- ‚ùå Code dupliqu√© entre importers (nested set, validation)
- ‚ùå Couplage fort avec les noms de tables

### 2.2 Transform System (G√©n√©rique)

#### Configuration Transform
```yaml
# transform.yml - D√©j√† g√©n√©rique !
- group_by: taxon  # ‚Üê R√©f√©rence la table "taxon_ref"
  sources:
    - name: occurrences
      data: occurrences  # ‚Üê R√©f√©rence la table "occurrences"
      grouping: taxon_ref  # ‚Üê Nom de table hardcod√©
      relation:
        plugin: nested_set
        key: taxon_ref_id
        fields:
          parent: parent_id  # ‚Üê Noms de colonnes hardcod√©s
          left: lft
          right: rght

  widgets_data:
    general_info:
      plugin: field_aggregator
      params:
        fields:
          - source: taxon_ref  # ‚Üê R√©f√©rence directe √† la table
            field: full_name
```

**Constat Crucial :**
Le syst√®me transform est "g√©n√©rique" au niveau des **plugins**, mais il d√©pend fortement des **noms de tables** et **noms de colonnes** fixes d√©finis lors de l'import.

**Probl√®mes de couplage :**
1. `group_by: taxon` ‚Üí Doit correspondre √† une table `taxon_ref`
2. `grouping: taxon_ref` ‚Üí Nom de table hardcod√©
3. `source: taxon_ref` ‚Üí R√©f√©rence directe √† la table
4. `fields: {parent_id, lft, rght}` ‚Üí Structure de nested set impos√©e

**Cons√©quence :**
Si on change le syst√®me d'import pour √™tre g√©n√©rique, **tous les fichiers transform.yml existants deviennent invalides** √† moins de :
- Maintenir les m√™mes noms de tables (taxon_ref, plot_ref, etc.)
- OU cr√©er un syst√®me de mapping/alias
- OU migrer automatiquement les configurations

### 2.3 Export System (G√©n√©rique)

Le syst√®me d'export est d√©j√† totalement g√©n√©rique car il consomme simplement les donn√©es transform√©es sans se soucier de leur origine.

**Pas d'impact majeur sur export**, mais il faut s'assurer que les donn√©es transform√©es restent dans le m√™me format.

---

## 3. Probl√®mes Identifi√©s & Impact

### 3.1 Probl√®me #1 : Couplage Fort avec Noms de Tables

**Sympt√¥me :**
```yaml
# Dans transform.yml
group_by: taxon  # ‚Üê "taxon" est mapp√© √† "taxon_ref" quelque part
source: taxon_ref  # ‚Üê R√©f√©rence directe
```

**Impact :**
- Impossible d'importer une entit√© nomm√©e "species" ou "locality" sans casser le syst√®me
- Les utilisateurs ne peuvent pas d√©finir leurs propres noms d'entit√©s
- Forte d√©pendance entre import.yml et transform.yml

**Gravit√© :** üî¥ CRITIQUE

### 3.2 Probl√®me #2 : Mod√®les SQLAlchemy Fixes

**Sympt√¥me :**
```python
# models.py
class TaxonRef(Base):
    __tablename__ = "taxon_ref"
    # Colonnes hardcod√©es
```

**Impact :**
- Impossible de cr√©er de nouvelles tables sans modifier le code Python
- Pas de flexibilit√© pour les structures de donn√©es custom
- Maintenance difficile (ajout de colonnes = migration Python)

**Gravit√© :** üî¥ CRITIQUE

### 3.3 Probl√®me #3 : Importers Sp√©cialis√©s

**Sympt√¥me :**
- `TaxonomyImporter` : 906 lignes
- `PlotImporter` : 1579 lignes
- Code dupliqu√© (nested sets, validation, progress tracking)

**Impact :**
- Maintenance co√ªteuse (bug fixes en 3+ endroits)
- Incoh√©rence potentielle entre importers
- Ajout d'un nouveau type = copier/coller 1000+ lignes

**Gravit√© :** üü° MAJEUR

### 3.4 Probl√®me #4 : Configuration Import Non Standard

**Sympt√¥me :**
```yaml
# import.yml - Structure custom pour chaque type
taxonomy:
  hierarchy:
    levels: [...]
plots:
  type: csv
  identifier: id_plot
occurrences:
  identifier: id_taxonref
```

**Impact :**
- Pas de pattern unifi√©
- Difficile √† documenter et expliquer
- GUI complexe (interface diff√©rente par type)

**Gravit√© :** üü° MAJEUR

### 3.5 Probl√®me #5 : Relations Hi√©rarchiques Hardcod√©es

**Sympt√¥me :**
```python
# Nested Set dans TaxonRef et PlotRef
lft = Column(Integer)
rght = Column(Integer)
level = Column(Integer)
parent_id = Column(Integer, ForeignKey("taxon_ref.id"))
```

**Impact :**
- Impossible d'utiliser d'autres types de hi√©rarchies (adjacency list simple, closure table)
- Logique nested set dupliqu√©e dans TaxonomyImporter et PlotImporter
- Difficile √† g√©n√©raliser

**Gravit√© :** üü† MOYEN

---

## 4. Vision Cible

### 4.1 Principes Directeurs

1. **User-Defined Entities** : L'utilisateur d√©finit ses propres entit√©s (pas nous)
2. **Convention over Configuration** : Auto-d√©tection intelligente avec possibilit√© d'override
3. **Unified Configuration** : M√™me structure YAML pour import/transform/export
4. **Config-first** : Tout d√©coule de la configuration d√©clarative
5. **Composable Pipeline** : Import en √©tapes (profile ‚Üí validate ‚Üí ingest ‚Üí relations ‚Üí enrich ‚Üí index)
6. **Metadata Everywhere** : Provenance, transformations, couverture FK, statistiques
7. **Fail Loud, Fail Early** : Validation avant mutation DB, transactions quand possible

### 4.2 Entity Registry Architecture

**Composant Central : Entity Registry**

Le Registry est le service central qui g√®re toutes les m√©tadonn√©es des entit√©s et sert de point d'acc√®s unique pour import, transform, export et GUI.

#### 4.2.1 Responsabilit√©s

```python
class EntityRegistry:
    """
    Central metadata service for all entities in Niamoto

    Responsibilities:
    - Register and persist entity metadata
    - Provide entity lookup by name/type
    - Manage table naming conventions
    - Track entity lifecycle state
    - Store schema versions and checksums
    """

    def register(self, entity: EntityMetadata) -> None:
        """Register a new entity with metadata"""

    def get(self, name: str) -> Optional[EntityMetadata]:
        """Get entity metadata by name"""

    def list_references(self) -> List[EntityMetadata]:
        """List all reference entities"""

    def list_datasets(self) -> List[EntityMetadata]:
        """List all dataset entities"""

    def get_relationships(self, entity_name: str) -> List[Relationship]:
        """Get all relationships for an entity"""

    def update_state(self, entity_name: str, state: EntityState) -> None:
        """Update entity lifecycle state"""

    def persist(self) -> None:
        """Persist registry to database"""

@dataclass
class EntityMetadata:
    """Metadata for a single entity"""
    name: str
    entity_type: EntityType  # reference | dataset
    kind: Optional[str]  # hierarchical | spatial | categorical
    table_name: str  # Actual DB table name
    connector_config: ConnectorConfig
    schema: EntitySchema
    relationships: List[Relationship]
    state: EntityState
    version: str
    checksum: str
    created_at: datetime
    updated_at: datetime

class EntityType(Enum):
    REFERENCE = "reference"
    DATASET = "dataset"

class EntityState(Enum):
    PLANNED = "planned"
    LOADING = "loading"
    READY = "ready"
    FAILED = "failed"

@dataclass
class EntitySchema:
    """Schema definition for an entity"""
    id_field: str
    fields: List[FieldDefinition]
    indexes: List[IndexDefinition]
    constraints: List[ConstraintDefinition]

@dataclass
class FieldDefinition:
    name: str
    type: str  # integer, float, string, text, date, geometry, json
    semantic: Optional[str]  # taxonomy.family, identifier, coordinates, etc.
    nullable: bool = True
    default: Optional[Any] = None

@dataclass
class Relationship:
    """Relationship between entities"""
    from_entity: str
    from_field: str
    to_entity: str
    to_field: str
    type: RelationType  # foreign_key | many_to_many

class RelationType(Enum):
    FOREIGN_KEY = "foreign_key"
    MANY_TO_MANY = "many_to_many"
```

#### 4.2.2 Persistence

**Metadata Table Schema (DuckDB):**

```sql
CREATE TABLE niamoto_metadata.entities (
    name VARCHAR PRIMARY KEY,
    entity_type VARCHAR NOT NULL,  -- 'reference' | 'dataset'
    kind VARCHAR,  -- 'hierarchical' | 'spatial' | 'categorical'
    table_name VARCHAR NOT NULL UNIQUE,
    connector_type VARCHAR NOT NULL,
    connector_config JSON NOT NULL,
    schema JSON NOT NULL,  -- Fields, indexes, constraints
    relationships JSON,
    state VARCHAR NOT NULL DEFAULT 'planned',
    version VARCHAR NOT NULL,
    checksum VARCHAR NOT NULL,
    created_at TIMESTAMP NOT NULL,
    updated_at TIMESTAMP NOT NULL,
    metadata JSON  -- Additional metadata
);

CREATE INDEX idx_entities_type ON niamoto_metadata.entities(entity_type);
CREATE INDEX idx_entities_state ON niamoto_metadata.entities(state);
```

#### 4.2.3 Table Naming Convention

Pour √©viter les collisions avec tables internes et garantir la clart√© :

```python
class TableNamingStrategy:
    """Consistent table naming across Niamoto"""

    @staticmethod
    def for_reference(entity_name: str) -> str:
        """entity_<name> for references"""
        return f"entity_{entity_name}"

    @staticmethod
    def for_dataset(dataset_name: str) -> str:
        """dataset_<name> for datasets"""
        return f"dataset_{dataset_name}"

    @staticmethod
    def is_internal(table_name: str) -> bool:
        """Check if table is internal (niamoto_*)"""
        return table_name.startswith("niamoto_")

# Examples:
# species (reference) ‚Üí entity_species
# sites (reference) ‚Üí entity_sites
# observations (dataset) ‚Üí dataset_observations
# niamoto_metadata.entities (internal) ‚Üí niamoto_metadata.entities
```

**Avantages :**
- ‚úÖ √âvite collisions avec tables syst√®me
- ‚úÖ Distinction claire reference vs dataset
- ‚úÖ Pr√©visible pour transform/export
- ‚úÖ Compatible avec conventions SQL

#### 4.2.4 Integration avec Transform/Export

**Avant (couplage fort) :**
```python
# Transform plugin hardcod√©
taxon_data = db.query("SELECT * FROM taxon_ref")
```

**Apr√®s (via Registry) :**
```python
# Transform plugin dynamique
registry = EntityRegistry.load()
entity = registry.get("species")  # or whatever user named it
taxon_data = db.query(f"SELECT * FROM {entity.table_name}")

# OR avec alias pour backward compat
taxonomy_entity = registry.get_by_alias("taxonomy")  # Points to user's chosen entity
```

### 4.3 Import Planner & Connectors

#### 4.3.1 Import Planner

Le Planner valide la configuration, r√©sout les d√©pendances, et cr√©e un plan d'ex√©cution ordonn√©.

```python
class ImportPlanner:
    """
    Validates configuration and creates executable import plan

    Responsibilities:
    - Validate configuration syntax and semantics
    - Resolve entity dependencies (FK order)
    - Detect circular dependencies
    - Create ordered execution plan
    - Estimate resources needed
    """

    def __init__(self, registry: EntityRegistry):
        self.registry = registry

    def create_plan(self, config: ImportConfig) -> ImportPlan:
        """
        Create executable import plan from configuration

        Steps:
        1. Validate configuration syntax (Pydantic)
        2. Validate semantic constraints (fields exist, types compatible)
        3. Build dependency graph
        4. Topological sort for execution order
        5. Create ImportPlan with ordered actions
        """
        # Validate
        validation = self._validate_config(config)
        if not validation.valid:
            raise ConfigurationError(validation.errors)

        # Resolve dependencies
        dependency_graph = self._build_dependency_graph(config)
        execution_order = self._topological_sort(dependency_graph)

        # Create plan
        actions = []
        for entity_name in execution_order:
            entity_config = config.get_entity(entity_name)
            actions.append(self._create_import_action(entity_config))

        return ImportPlan(
            actions=actions,
            estimated_duration=self._estimate_duration(actions),
            resource_requirements=self._estimate_resources(actions)
        )

    def _validate_config(self, config: ImportConfig) -> ValidationResult:
        """Comprehensive configuration validation"""
        errors = []
        warnings = []

        # Check references before datasets
        for dataset in config.datasets.values():
            for link in dataset.links:
                if link.entity not in config.references:
                    errors.append(f"Dataset {dataset.name} references unknown entity {link.entity}")

        # Check for circular dependencies
        if self._has_circular_dependencies(config):
            errors.append("Circular dependencies detected in entity relationships")

        # Check connector availability
        for entity in config.all_entities():
            if not self._connector_available(entity.connector.type):
                errors.append(f"Connector {entity.connector.type} not available")

        return ValidationResult(valid=len(errors) == 0, errors=errors, warnings=warnings)

    def _build_dependency_graph(self, config: ImportConfig) -> DependencyGraph:
        """Build graph of entity dependencies"""
        graph = DependencyGraph()

        # Add all entities as nodes
        for entity in config.all_entities():
            graph.add_node(entity.name)

        # Add edges for relationships
        for dataset in config.datasets.values():
            for link in dataset.links:
                # Dataset depends on reference
                graph.add_edge(dataset.name, link.entity)

        return graph

    def _topological_sort(self, graph: DependencyGraph) -> List[str]:
        """Topological sort to determine import order"""
        # Standard Kahn's algorithm
        # References before datasets that depend on them
        pass

@dataclass
class ImportPlan:
    """Executable import plan"""
    actions: List[ImportAction]
    estimated_duration: timedelta
    resource_requirements: ResourceEstimate
    created_at: datetime

    def execute(self, engine: ImportEngine) -> ImportResult:
        """Execute the plan"""
        pass

@dataclass
class ImportAction:
    """Single import action (one entity)"""
    entity_name: str
    connector: Connector
    target_table: str
    mode: ImportMode  # create | replace | append
    chunk_size: int
    checkpoint_enabled: bool

class ImportMode(Enum):
    CREATE = "create"
    REPLACE = "replace"
    APPEND = "append"
    UPDATE = "update"
```

#### 4.3.2 Connector Architecture

Les Connectors abstraient les sources de donn√©es avec une interface unifi√©e.

```python
from abc import ABC, abstractmethod
from typing import Iterator, Optional

class Connector(ABC):
    """
    Abstract base class for data connectors

    Connectors handle:
    - Connection to data source
    - Schema detection/profiling
    - Data streaming in chunks
    - Error handling and retries
    """

    @abstractmethod
    def connect(self, config: ConnectorConfig) -> Connection:
        """Establish connection to data source"""
        pass

    @abstractmethod
    def profile(self, connection: Connection) -> SourceProfile:
        """Profile data source (schema, stats, samples)"""
        pass

    @abstractmethod
    def stream(self, connection: Connection, chunk_size: int) -> Iterator[DataChunk]:
        """Stream data in chunks"""
        pass

    @abstractmethod
    def validate(self, connection: Connection) -> ValidationResult:
        """Validate data source"""
        pass

    def close(self, connection: Connection) -> None:
        """Close connection (optional)"""
        pass

@dataclass
class ConnectorConfig:
    """Configuration for a connector"""
    type: str
    params: Dict[str, Any]

@dataclass
class Connection:
    """Active connection to data source"""
    connector_type: str
    handle: Any  # Connector-specific handle
    metadata: Dict[str, Any]

@dataclass
class SourceProfile:
    """Profile of a data source"""
    row_count: int
    columns: List[ColumnProfile]
    sample_rows: List[Dict]
    statistics: Dict[str, Any]

@dataclass
class DataChunk:
    """Chunk of data from source"""
    data: Any  # DataFrame, List[Dict], etc.
    chunk_index: int
    total_chunks: int
    row_count: int
```

#### 4.3.3 Concrete Connectors

**DuckDB CSV Connector (Natif) :**

```python
class DuckDBCSVConnector(Connector):
    """Connector for CSV files using DuckDB native functions"""

    def connect(self, config: ConnectorConfig) -> Connection:
        """No actual connection needed, DuckDB reads directly"""
        path = config.params['path']
        if not Path(path).exists():
            raise FileNotFoundError(f"CSV file not found: {path}")

        return Connection(
            connector_type='duckdb_csv',
            handle=path,
            metadata={'format': 'csv'}
        )

    def profile(self, connection: Connection) -> SourceProfile:
        """Use DuckDB schema detection"""
        path = connection.handle

        # DuckDB auto-detects schema
        schema_df = self.db.execute(f"""
            DESCRIBE SELECT * FROM read_csv_auto('{path}')
        """).fetchdf()

        # Get sample rows
        sample_df = self.db.execute(f"""
            SELECT * FROM read_csv_auto('{path}') LIMIT 100
        """).fetchdf()

        # Get count
        count = self.db.execute(f"""
            SELECT COUNT(*) FROM read_csv_auto('{path}')
        """).fetchone()[0]

        columns = [
            ColumnProfile(
                name=row['column_name'],
                type=self._map_duckdb_type(row['column_type']),
                nullable=row['null'] == 'YES',
                unique_ratio=None,  # Could compute if needed
                null_ratio=None
            )
            for _, row in schema_df.iterrows()
        ]

        return SourceProfile(
            row_count=count,
            columns=columns,
            sample_rows=sample_df.head(10).to_dict('records'),
            statistics={'detected_by': 'duckdb_csv_auto'}
        )

    def stream(self, connection: Connection, chunk_size: int) -> Iterator[DataChunk]:
        """Stream CSV in chunks using DuckDB"""
        path = connection.handle

        # Get total count
        total = self.db.execute(f"""
            SELECT COUNT(*) FROM read_csv_auto('{path}')
        """).fetchone()[0]

        total_chunks = (total // chunk_size) + (1 if total % chunk_size else 0)

        # Stream chunks
        for i in range(total_chunks):
            offset = i * chunk_size
            chunk_df = self.db.execute(f"""
                SELECT * FROM read_csv_auto('{path}')
                LIMIT {chunk_size} OFFSET {offset}
            """).fetchdf()

            yield DataChunk(
                data=chunk_df,
                chunk_index=i,
                total_chunks=total_chunks,
                row_count=len(chunk_df)
            )

    def validate(self, connection: Connection) -> ValidationResult:
        """Validate CSV can be read"""
        try:
            path = connection.handle
            self.db.execute(f"SELECT * FROM read_csv_auto('{path}') LIMIT 1")
            return ValidationResult(valid=True, errors=[])
        except Exception as e:
            return ValidationResult(valid=False, errors=[str(e)])
```

**Spatial Connector (DuckDB Spatial Extension) :**

```python
class DuckDBSpatialConnector(Connector):
    """Connector for spatial files using DuckDB spatial extension"""

    def __init__(self, db):
        self.db = db
        # Ensure spatial extension is loaded
        self.db.execute("INSTALL spatial")
        self.db.execute("LOAD spatial")

    def connect(self, config: ConnectorConfig) -> Connection:
        """Connect to spatial file"""
        path = config.params['path']
        format = config.params.get('format', 'geojson')

        if not Path(path).exists():
            raise FileNotFoundError(f"Spatial file not found: {path}")

        return Connection(
            connector_type='duckdb_spatial',
            handle=path,
            metadata={'format': format}
        )

    def profile(self, connection: Connection) -> SourceProfile:
        """Profile spatial data"""
        path = connection.handle

        # Read with ST_Read
        schema_df = self.db.execute(f"""
            DESCRIBE SELECT * FROM ST_Read('{path}')
        """).fetchdf()

        # Sample
        sample_df = self.db.execute(f"""
            SELECT *, ST_AsText(geometry) as geometry_wkt
            FROM ST_Read('{path}')
            LIMIT 10
        """).fetchdf()

        # Count
        count = self.db.execute(f"""
            SELECT COUNT(*) FROM ST_Read('{path}')
        """).fetchone()[0]

        columns = [
            ColumnProfile(
                name=row['column_name'],
                type='geometry' if row['column_name'] == 'geometry' else self._map_duckdb_type(row['column_type']),
                nullable=row['null'] == 'YES'
            )
            for _, row in schema_df.iterrows()
        ]

        return SourceProfile(
            row_count=count,
            columns=columns,
            sample_rows=sample_df.to_dict('records'),
            statistics={'spatial': True}
        )

    def stream(self, connection: Connection, chunk_size: int) -> Iterator[DataChunk]:
        """Stream spatial data"""
        path = connection.handle

        # Similar to CSV but with ST_Read
        total = self.db.execute(f"""
            SELECT COUNT(*) FROM ST_Read('{path}')
        """).fetchone()[0]

        total_chunks = (total // chunk_size) + (1 if total % chunk_size else 0)

        for i in range(total_chunks):
            offset = i * chunk_size
            chunk_df = self.db.execute(f"""
                SELECT * FROM ST_Read('{path}')
                LIMIT {chunk_size} OFFSET {offset}
            """).fetchdf()

            yield DataChunk(
                data=chunk_df,
                chunk_index=i,
                total_chunks=total_chunks,
                row_count=len(chunk_df)
            )
```

**Connector Registry :**

```python
class ConnectorRegistry:
    """Registry of available connectors"""

    _connectors: Dict[str, Type[Connector]] = {}

    @classmethod
    def register(cls, connector_type: str, connector_class: Type[Connector]):
        """Register a connector"""
        cls._connectors[connector_type] = connector_class

    @classmethod
    def get(cls, connector_type: str) -> Type[Connector]:
        """Get connector class by type"""
        if connector_type not in cls._connectors:
            raise ValueError(f"Unknown connector type: {connector_type}")
        return cls._connectors[connector_type]

    @classmethod
    def list_available(cls) -> List[str]:
        """List available connector types"""
        return list(cls._connectors.keys())

# Register connectors
ConnectorRegistry.register('duckdb_csv', DuckDBCSVConnector)
ConnectorRegistry.register('duckdb_spatial', DuckDBSpatialConnector)
```

### 4.4 Lifecycle & Observability

#### 4.4.1 Import Job Lifecycle

**State Machine :**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PLANNED ‚îÇ ‚Üê Config validated, plan created
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LOADING ‚îÇ ‚Üê Import in progress, checkpoints possible
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚îú‚îÄ‚Üí ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ   ‚îÇ READY ‚îÇ ‚Üê Import completed successfully
     ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚îî‚îÄ‚Üí ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ FAILED ‚îÇ ‚Üê Import failed, can be retried
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Implementation :**

```python
class ImportJob:
    """
    Represents an import job with lifecycle management

    Features:
    - State tracking
    - Checkpointing for resume
    - Progress reporting
    - Error recovery
    """

    def __init__(self, plan: ImportPlan, registry: EntityRegistry):
        self.plan = plan
        self.registry = registry
        self.state = EntityState.PLANNED
        self.checkpoints: List[Checkpoint] = []
        self.errors: List[ImportError] = []
        self.started_at: Optional[datetime] = None
        self.completed_at: Optional[datetime] = None

    def execute(self, engine: ImportEngine) -> ImportResult:
        """Execute the import job"""
        try:
            self._transition_to(EntityState.LOADING)
            self.started_at = datetime.now()

            for action in self.plan.actions:
                # Execute action
                result = engine.execute_action(action)

                # Create checkpoint
                checkpoint = Checkpoint(
                    entity_name=action.entity_name,
                    completed_at=datetime.now(),
                    rows_imported=result.rows_imported,
                    state=result.state
                )
                self.checkpoints.append(checkpoint)

                # Update registry
                self.registry.update_state(action.entity_name, EntityState.READY)

            self._transition_to(EntityState.READY)
            self.completed_at = datetime.now()

            return ImportResult(
                success=True,
                entities_imported=len(self.plan.actions),
                total_rows=sum(cp.rows_imported for cp in self.checkpoints),
                duration=self.completed_at - self.started_at
            )

        except Exception as e:
            self._transition_to(EntityState.FAILED)
            self.errors.append(ImportError(
                entity=action.entity_name,
                error=str(e),
                timestamp=datetime.now()
            ))
            raise

    def resume(self) -> ImportResult:
        """Resume failed import from last checkpoint"""
        if self.state != EntityState.FAILED:
            raise ValueError("Can only resume failed jobs")

        # Find last successful checkpoint
        last_checkpoint = self.checkpoints[-1] if self.checkpoints else None

        # Create new plan from remaining actions
        remaining_actions = [
            action for action in self.plan.actions
            if not any(cp.entity_name == action.entity_name for cp in self.checkpoints)
        ]

        if not remaining_actions:
            raise ValueError("No remaining actions to resume")

        # Execute remaining actions
        # ...

    def _transition_to(self, new_state: EntityState):
        """Transition to new state"""
        logger.info(f"Import job transitioning: {self.state} ‚Üí {new_state}")
        self.state = new_state

@dataclass
class Checkpoint:
    """Import checkpoint for recovery"""
    entity_name: str
    completed_at: datetime
    rows_imported: int
    state: EntityState
    metadata: Dict[str, Any] = None

@dataclass
class ImportError:
    """Error during import"""
    entity: str
    error: str
    timestamp: datetime
    traceback: Optional[str] = None
```

#### 4.4.2 Observability

**Structured Logging :**

```python
import structlog

logger = structlog.get_logger()

class ObservableImportEngine:
    """Import engine with comprehensive observability"""

    def execute_action(self, action: ImportAction) -> ActionResult:
        """Execute import action with logging"""

        # Start context
        log = logger.bind(
            entity=action.entity_name,
            connector=action.connector.type,
            mode=action.mode.value,
            job_id=self.job_id
        )

        log.info("import_action_started",
                 target_table=action.target_table,
                 chunk_size=action.chunk_size)

        start_time = time.time()
        rows_processed = 0

        try:
            # Execute import
            for chunk in action.connector.stream(chunk_size=action.chunk_size):
                # Process chunk
                rows_processed += chunk.row_count

                log.debug("chunk_processed",
                         chunk_index=chunk.chunk_index,
                         rows=chunk.row_count,
                         total_rows=rows_processed)

                # Emit metric
                self.metrics.increment('rows_imported', chunk.row_count,
                                      tags={'entity': action.entity_name})

            duration = time.time() - start_time

            log.info("import_action_completed",
                    rows_imported=rows_processed,
                    duration_seconds=duration,
                    rows_per_second=rows_processed / duration if duration > 0 else 0)

            # Emit metrics
            self.metrics.timing('import_duration', duration,
                               tags={'entity': action.entity_name})
            self.metrics.gauge('entity_row_count', rows_processed,
                             tags={'entity': action.entity_name})

            return ActionResult(
                success=True,
                rows_imported=rows_processed,
                duration=duration,
                state=EntityState.READY
            )

        except Exception as e:
            log.error("import_action_failed",
                     error=str(e),
                     rows_processed=rows_processed,
                     exc_info=True)

            # Emit error metric
            self.metrics.increment('import_errors',
                                  tags={'entity': action.entity_name, 'error_type': type(e).__name__})

            raise
```

**Metrics & Progress Tracking :**

```python
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn

class ProgressTracker:
    """Rich progress tracker for CLI"""

    def __init__(self):
        self.progress = Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TextColumn("[bold green]{task.fields[status]}"),
        )

    def track_import(self, job: ImportJob):
        """Track import job progress"""
        with self.progress:
            # Overall task
            overall_task = self.progress.add_task(
                "Import Job",
                total=len(job.plan.actions),
                status="Starting..."
            )

            for action in job.plan.actions:
                # Entity task
                entity_task = self.progress.add_task(
                    f"Importing {action.entity_name}",
                    total=100,  # Will update as we know chunk count
                    status="Connecting..."
                )

                # Execute and update
                for chunk in action.connector.stream(chunk_size=action.chunk_size):
                    self.progress.update(
                        entity_task,
                        completed=chunk.chunk_index,
                        total=chunk.total_chunks,
                        status=f"{chunk.row_count} rows"
                    )

                self.progress.update(entity_task, status="‚úì Complete")
                self.progress.advance(overall_task)
```

### 4.5 Configuration Cible

**Structure references/datasets (align√©e avec refactor-roadmap) :**

```yaml
# import.yml - Nouvelle structure g√©n√©rique
entities:
  references:
    species:
      kind: hierarchical
      connector:
        type: duckdb_csv
        path: data/species.csv
      schema:
        id_field: species_id
        fields:
          - name: family
            type: string
            semantic: taxonomy.family
          - name: genus
            type: string
            semantic: taxonomy.genus
          - name: species
            type: string
            semantic: taxonomy.species
      hierarchy:
        strategy: recursive_cte  # DuckDB recursive CTEs
        levels: [family, genus, species]
      enrichment:
        - plugin: gbif_enricher
          config:
            url: https://api.gbif.org
            key_env: GBIF_API_KEY

    sites:
      kind: spatial
      connector:
        type: duckdb_spatial
        path: data/sites.geojson
      schema:
        id_field: site_id
        fields:
          - name: site_name
            type: string
          - name: geometry
            type: geometry
            srid: 4326

    habitats:
      kind: categorical
      connector:
        type: duckdb_csv
        path: data/habitats.csv
      schema:
        id_field: habitat_code
        fields:
          - name: habitat_type
            type: string
          - name: description
            type: text

  datasets:
    observations:
      connector:
        type: duckdb_csv
        path: data/observations.csv
      schema:
        id_field: occurrence_id
        fields:
          - name: species_code
            type: string
            reference: species.species_id
          - name: site_code
            type: string
            reference: sites.site_id
          - name: habitat_code
            type: string
            reference: habitats.habitat_code
          - name: dbh
            type: float
          - name: height
            type: float
          - name: observation_date
            type: date
      links:
        - entity: species
          field: species_code
          target_field: species_id
        - entity: sites
          field: site_code
          target_field: site_id
        - entity: habitats
          field: habitat_code
          target_field: habitat_code
      options:
        mode: replace
        chunk_size: 10000
```

**Avantages de cette structure :**
- ‚úÖ Distinction claire `references` vs `datasets`
- ‚úÖ Connectors explicites (DuckDB CSV, Spatial)
- ‚úÖ Hi√©rarchies via recursive CTEs (pas de nested sets!)
- ‚ö†Ô∏è Migration √† planifier pour les widgets/transformations qui utilisent encore `lft`/`rght` (proposer alias ou vue transitoire avant suppression).
- üìå Plugins √† adapter : `hierarchical_nav_widget`, `geospatial_extractor`, `top_ranking` (et `nested_set` loader / export HTML pour la pr√©paration).
- ‚úÖ Semantic types pour d√©tection intelligente
- ‚úÖ Liens FK clairs et validables
- ‚úÖ Enrichment plugins optionnels

### 4.6 Sch√©ma de Base de Donn√©es Cible (DuckDB)

**Convention de nommage :** `entity_<name>` pour references, `dataset_<name>` pour datasets

```sql
-- Tables cr√©√©es dynamiquement par ImportEngine

-- Reference: species (hierarchical) avec Recursive CTEs (pas de nested sets!)
CREATE TABLE entity_species (
    id INTEGER PRIMARY KEY,
    species_id VARCHAR,  -- ID externe
    scientific_name VARCHAR,
    family VARCHAR,
    genus VARCHAR,
    species VARCHAR,
    parent_id INTEGER REFERENCES entity_species(id),
    extra_data JSON
);

-- Index pour performance hierarchical queries
CREATE INDEX idx_entity_species_parent ON entity_species(parent_id);
CREATE INDEX idx_entity_species_family ON entity_species(family);
CREATE INDEX idx_entity_species_genus ON entity_species(genus);

-- Queries hi√©rarchiques avec Recursive CTEs (simple!)
WITH RECURSIVE descendants AS (
    SELECT * FROM entity_species WHERE id = 42
    UNION ALL
    SELECT s.* FROM entity_species s
    JOIN descendants d ON s.parent_id = d.id
)
SELECT * FROM descendants;

-- Reference: sites (spatial)
CREATE TABLE entity_sites (
    id INTEGER PRIMARY KEY,
    site_id VARCHAR,
    site_name VARCHAR,
    geometry GEOMETRY,  -- DuckDB spatial extension
    extra_data JSON
);

-- Index spatial avec DuckDB spatial extension
CREATE INDEX idx_entity_sites_geom ON entity_sites USING RTREE(geometry);

-- Reference: habitats (categorical)
CREATE TABLE entity_habitats (
    id INTEGER PRIMARY KEY,
    habitat_code VARCHAR,
    habitat_type VARCHAR,
    description TEXT,
    extra_data JSON
);

-- Dataset: observations (factual data)
CREATE TABLE dataset_observations (
    id INTEGER PRIMARY KEY,
    occurrence_id VARCHAR,
    species_id INTEGER REFERENCES entity_species(id),
    site_id INTEGER REFERENCES entity_sites(id),
    habitat_id INTEGER REFERENCES entity_habitats(id),
    -- Toutes les colonnes du CSV source
    dbh DOUBLE,
    height DOUBLE,
    observation_date DATE,
    extra_data JSON
);

-- Index pour FK lookups
CREATE INDEX idx_dataset_observations_species ON dataset_observations(species_id);
CREATE INDEX idx_dataset_observations_site ON dataset_observations(site_id);
CREATE INDEX idx_dataset_observations_habitat ON dataset_observations(habitat_id);
CREATE INDEX idx_dataset_observations_date ON dataset_observations(observation_date);
```

**Avantages DuckDB :**
- ‚úÖ Pas de `AUTOINCREMENT` complexe, juste `INTEGER PRIMARY KEY`
- ‚úÖ Types natifs : `VARCHAR`, `DOUBLE`, `DATE`, `GEOMETRY`, `JSON`
- ‚úÖ Recursive CTEs au lieu de nested sets (lft/rght)
- ‚úÖ Spatial extension native pour g√©om√©tries
- ‚úÖ Compression automatique (bases 5-10x plus petites)
- ‚úÖ Performance analytique sup√©rieure (10-100x sur agr√©gations)

### 4.7 Configuration Transform Cible

**Int√©gration avec Entity Registry :**

```yaml
# transform.yml - Compatible avec nouveau syst√®me via Registry
- group_by: species  # ‚Üê R√©f√©rence l'entit√© "species"
  sources:
    - name: observations
      data: dataset_observations  # ‚Üê Registry r√©sout vers "dataset_observations"
      grouping: entity_species  # ‚Üê Registry r√©sout vers "entity_species"
      relation:
        plugin: recursive_hierarchy  # ‚Üê Nouveau plugin pour DuckDB recursive CTEs
        key: species_id

  widgets_data:
    general_info:
      plugin: field_aggregator
      params:
        fields:
          - source: entity_species  # ‚Üê Via Registry
            field: scientific_name  # ‚Üê Champ d√©fini dans import.yml
            target: name
          - source: entity_species
            field: family
            target: family_name
```

**R√©solution via Registry :**

```python
# Dans le transformer
registry = EntityRegistry.load()
species_entity = registry.get("species")  # EntityMetadata
# species_entity.table_name = "entity_species"

observations_entity = registry.get("observations")  # EntityMetadata
# observations_entity.table_name = "dataset_observations"

# Query building devient:
query = f"""
    SELECT
        {species_entity.table_name}.scientific_name,
        COUNT(*) as occurrence_count
    FROM {observations_entity.table_name}
    JOIN {species_entity.table_name}
        ON {observations_entity.table_name}.species_id = {species_entity.table_name}.id
    GROUP BY {species_entity.table_name}.id
"""
```

**Point cl√© :**
- Les noms d'entit√©s dans `import.yml` sont symboliques
- Le Registry les mappe vers les vrais noms de tables
- Transform/Export utilisent le Registry, jamais de hardcoded table names

---

## 5. Analyse des Risques & Contraintes

### 5.1 Risques Techniques

| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|--------|------------|
| **Performance d√©grad√©e** | üü° MOYENNE | üü° MAJEUR | Indexes dynamiques + Benchmarking continu |
| **Bugs sur cas edge** | üü° MOYENNE | üü° MAJEUR | Tests exhaustifs + Cas de test vari√©s |
| **Complexit√© accrue** | üü† MOYENNE | üü† MOYEN | Documentation claire + Abstractions simples |
| **Over-engineering** | üü† MOYENNE | üü† MOYEN | It√©rations courtes + Validation utilisateur |
| **Couplage persistant des plugins (Config/DB)** | üü° MOYENNE | üü† MAJEUR | Migrer vers le Registry
et l'adaptateur DuckDB durant la refonte (√©viter un refactor interm√©diaire inutile).

### 5.2 Contraintes Business

1. **Timeline** : Besoin d'une solution viable en 8 semaines
2. **Utilisateurs** : Scientifiques non-techniques qui ont besoin de simplicit√©
3. **Documentation** : Doit √™tre mise √† jour en parall√®le avec le d√©veloppement
4. **Testing** : N√©cessite datasets vari√©s pour validation

### 5.3 Contraintes Techniques (avec DuckDB)

1. ~~**SQLite** : Limitations sur ALTER TABLE complexe~~ ‚Üí **DuckDB** : DDL flexible avec CREATE OR REPLACE
2. ~~**Nested Sets** : Algorithme complexe, difficile √† g√©n√©raliser~~ ‚Üí **Recursive CTEs** : Simple et performant
3. ~~**G√©om√©tries** : Validation et stockage WKT n√©cessitent traitement sp√©cial~~ ‚Üí **Spatial Extension** : Support natif
4. **Hi√©rarchies** : Peuvent √™tre profondes (8+ niveaux), performance critique ‚Üí Recursive CTEs optimis√©s
5. **Dynamic Schema** : Cr√©ation de tables/indexes √† la vol√©e n√©cessite validation rigoureuse ‚Üí DuckDB schema detection aide

**Migration DuckDB √©limine la majorit√© des contraintes techniques!**

---

## 6. Solution Propos√©e

**Approche : Refactoring Direct en 3 Phases**

Sans contrainte de r√©trocompatibilit√©, nous pouvons adopter une approche plus directe et efficace :

### Vue d'Ensemble

**Timeline :** 8 semaines (au lieu de 12)
**Strat√©gie :** Remplacer compl√®tement le syst√®me existant par une architecture g√©n√©rique propre

### Architecture Cible (avec DuckDB)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Configuration Layer                         ‚îÇ
‚îÇ  ImportConfig (Pydantic) ‚Üí references + datasets                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Entity Registry                            ‚îÇ
‚îÇ  - EntityMetadata (name, type, table_name, schema, state)       ‚îÇ
‚îÇ  - Persistence (niamoto_metadata.entities)                      ‚îÇ
‚îÇ  - Table naming (entity_*, dataset_*)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Import Planner                             ‚îÇ
‚îÇ  - Validate config (syntax + semantics)                         ‚îÇ
‚îÇ  - Build dependency graph                                       ‚îÇ
‚îÇ  - Topological sort (execution order)                           ‚îÇ
‚îÇ  - Create ImportPlan (ordered actions)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Import Engine                              ‚îÇ
‚îÇ  - Execute ImportPlan                                           ‚îÇ
‚îÇ  - Connectors (DuckDB CSV, Spatial, API)                       ‚îÇ
‚îÇ  - Stream data in chunks                                        ‚îÇ
‚îÇ  - Create tables (entity_*, dataset_*)                          ‚îÇ
‚îÇ  - Enforce relationships                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Observability                              ‚îÇ
‚îÇ  - ImportJob (state machine: planned ‚Üí loading ‚Üí ready/failed) ‚îÇ
‚îÇ  - Structured logging (structlog)                               ‚îÇ
‚îÇ  - Metrics (rows_imported, duration, errors)                    ‚îÇ
‚îÇ  - Progress tracking (Rich)                                     ‚îÇ
‚îÇ  - Checkpoints (for resume)                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      DuckDB Store                               ‚îÇ
‚îÇ  - Tables: entity_*, dataset_*, niamoto_metadata.*              ‚îÇ
‚îÇ  - Recursive CTEs (hierarchies)                                 ‚îÇ
‚îÇ  - Spatial extension (geometry)                                 ‚îÇ
‚îÇ  - Native CSV/Parquet ingestion                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚Üì                       ‚Üì
              Transform                  Export
           (via Registry)            (via Registry)
```

**Composants Cl√©s :**

1. **Entity Registry** : M√©tadonn√©es centralis√©es (voir 4.2)
2. **Import Planner** : Validation + r√©solution d√©pendances (voir 4.3)
3. **Connectors** : Abstractions sources de donn√©es (voir 4.3)
4. **Import Engine** : Ex√©cution plan avec observability (voir 4.4)
5. **DuckDB Store** : Base analytique performante
6. **Bootstrap System** : Profiling + g√©n√©ration config (Phase 2)
7. **GUI Integration** : Import wizard (Phase 3)

### Phases de D√©veloppement

#### Phase 1 : Entity Registry + Import Engine (3 semaines)

**Semaine 1 : Entity Registry & Config**
- Impl√©menter `EntityRegistry` avec persistence (DuckDB metadata table)
- Cr√©er `EntityMetadata`, `EntitySchema`, `FieldDefinition` (Pydantic)
- Table naming strategy (`entity_*`, `dataset_*`)
- Config loader pour nouvelle structure `references`/`datasets`
- Migration DuckDB : CREATE OR REPLACE, spatial extension setup

**Semaine 2 : Import Planner & Connectors**
- Impl√©menter `ImportPlanner` (validation, dependency resolution, topological sort)
- Cr√©er `Connector` interface + `DuckDBCSVConnector` (utilise `read_csv_auto`)
- Cr√©er `DuckDBSpatialConnector` (utilise `ST_Read`, spatial extension)
- `ConnectorRegistry` pour enregistrement connectors
- Tests validation config et r√©solution d√©pendances

**Semaine 3 : Import Engine & Observability**
- Impl√©menter `ImportEngine` avec ex√©cution plan
- `ImportJob` avec state machine (planned ‚Üí loading ‚Üí ready/failed)
- Structured logging (structlog), metrics, progress tracking (Rich)
- Checkpointing pour resume
- Tests end-to-end avec DuckDB

**Livrables Phase 1 :**
- ‚úÖ Entity Registry op√©rationnel avec DuckDB
- ‚úÖ Import Planner validant config et r√©solvant d√©pendances
- ‚úÖ Connectors DuckDB (CSV, Spatial)
- ‚úÖ Import Engine avec observability
- ‚úÖ Tests coverage > 85%
- ‚úÖ Performance DuckDB valid√©e (10-100x plus rapide que SQLite sur agr√©gations)

#### Phase 2 : Bootstrap & Auto-detection (3 semaines)

**Semaine 4 : Data Profiler**
- Impl√©menter `DataProfiler` avec d√©tection intelligente
- D√©tection types (hierarchical, spatial, factual)
- D√©tection relations (foreign keys)
- Scoring de confiance

**Semaine 5 : Config Generator**
- Impl√©menter `ConfigGenerator`
- G√©n√©ration automatique `import.yml`
- Validation coh√©rence
- Suggestions intelligentes

**Semaine 6 : CLI & Tests**
- CLI: `niamoto bootstrap <data_dir>`
- Tests sur datasets vari√©s
- Accuracy testing (> 85%)
- Documentation bootstrap process

**Livrables Phase 2 :**
- ‚úÖ Bootstrap automatique fonctionnel
- ‚úÖ Accuracy d√©tection > 85%
- ‚úÖ CLI user-friendly
- ‚úÖ Documentation compl√®te

#### Phase 3 : GUI Integration (2 semaines)

**Semaine 7 : Import Wizard**
- Composants React: Upload, Analysis, Review
- √âditeur visuel de configuration
- Preview des donn√©es
- Validation en temps r√©el

**Semaine 8 : Finalisation**
- Tests E2E complets
- Polish UI/UX
- Documentation utilisateur
- Video tutorial

**Livrables Phase 3 :**
- ‚úÖ GUI compl√®te et intuitive
- ‚úÖ User testing r√©ussi
- ‚úÖ Documentation compl√®te
- ‚úÖ Ready for production

### Avantages de Cette Approche

‚úÖ **Simplicit√©** : Pas de code legacy √† maintenir
‚úÖ **Rapidit√©** : 8 semaines au lieu de 12
‚úÖ **Qualit√©** : Architecture propre d√®s le d√©part
‚úÖ **Flexibilit√©** : Pas de contraintes historiques
‚úÖ **Testabilit√©** : Code neuf, facile √† tester

### Gestion du Risque

Sans r√©trocompatibilit√©, les risques sont r√©duits mais n√©cessitent quand m√™me attention :

**Strat√©gie de mitigation :**
1. **Tests exhaustifs** : Datasets vari√©s, cas edge
2. **Validation continue** : Benchmarking performance √† chaque phase
3. **Documentation parall√®le** : Docs √©crites pendant d√©veloppement
4. **User feedback early** : Tests utilisateurs d√®s Phase 2
5. **Rollback plan** : Possibilit√© de revenir √† l'ancien syst√®me si critique

---

## 7. Roadmap D√©taill√©e

La roadmap compl√®te est d√©crite dans la [Section 6](#6-solution-propos√©e). Cette section fournit uniquement des notes d'impl√©mentation critiques.

### Phase 1 : Core Abstractions + Generic Engine (Semaines 1-3)

**Composants cl√©s √† impl√©menter :**

1. **BaseEntityImporter** (`src/niamoto/core/imports/base.py`)
   - M√©thodes communes : `validate_file()`, `load_data()`, `batch_commit()`, `handle_extra_data()`
   - Gestion d'erreurs standardis√©e
   - Progress tracking int√©gr√©

2. **HierarchyManager** (`src/niamoto/core/imports/hierarchy.py`)
   - Support nested sets (prioritaire)
   - Interface pour futurs types de hi√©rarchies (adjacency list, closure table)
   - M√©thodes : `build_hierarchy()`, `update_nested_set_values()`, `validate_hierarchy()`

3. **GeometryValidator** (`src/niamoto/core/imports/geometry.py`)
   - Validation WKT/WKB/GeoJSON
   - Conversion vers WKT unifi√©
   - Agr√©gation de g√©om√©tries pour hi√©rarchies

4. **DynamicTableFactory** (`src/niamoto/core/imports/table_factory.py`)
   - Cr√©ation tables avec SQLAlchemy MetaData
   - Mapping types : `{integer, float, string, text, date, geometry, json}`
   - G√©n√©ration automatique d'indexes

5. **GenericEntityImporter** (`src/niamoto/core/imports/generic_importer.py`)
   - Utilise tous les composants ci-dessus
   - G√®re les 3 types d'entit√©s : hierarchical, spatial, categorical
   - Linking automatique entre entit√©s

**Points d'attention :**
- Benchmarking continu vs. ancien syst√®me
- Tests avec datasets r√©els d√®s semaine 2
- Documentation parall√®le au code

### Phase 2 : Bootstrap & Auto-detection (Semaines 4-6)

**Composants cl√©s √† impl√©menter :**

1. **DataProfiler** (`src/niamoto/core/imports/profiler.py`)
   - D√©tection patterns taxonomiques (binomial nomenclature)
   - D√©tection g√©om√©tries (WKT, coordonn√©es)
   - D√©tection identifiants (unique ratio > 0.95)
   - D√©tection relations (patterns `*_id`, `*_code`)

2. **ConfigGenerator** (`src/niamoto/core/imports/config_generator.py`)
   - G√©n√®re `import.yml` depuis profiles
   - Validation coh√©rence (r√©f√©rences utilis√©es, liens valides)
   - Scoring de confiance global

3. **CLI Bootstrap** (`src/niamoto/cli/commands/bootstrap.py`)
   - Command: `niamoto bootstrap <data_dir>`
   - Flags: `--auto-confirm`, `--output`, `--preview`
   - Workflow: Analyze ‚Üí Generate ‚Üí Preview ‚Üí Import

**Target accuracy :**
- D√©tection type d'entit√© : > 90%
- D√©tection relations : > 80%
- Configuration utilisable sans √©dition : > 70%

### Phase 3 : GUI Integration (Semaines 7-8)

**Composants cl√©s √† impl√©menter :**

1. **Backend API** (`src/niamoto/gui/api/routers/imports.py`)
   - `POST /api/imports/analyze` : Analyse fichiers upload√©s
   - `POST /api/imports/generate` : G√©n√®re configuration
   - `POST /api/imports/run` : Execute import
   - `GET /api/imports/status/{id}` : Status import en cours

2. **Frontend Wizard** (`src/niamoto/gui/ui/src/components/import/`)
   - `ImportWizard.tsx` : Container principal
   - `UploadStage.tsx` : Drag & drop files
   - `AnalysisStage.tsx` : Affichage profiles
   - `ReviewStage.tsx` : √âdition configuration
   - `ConfigEditor.tsx` : √âditeur visuel YAML
   - `ImportProgressStage.tsx` : Progress real-time

**User testing :**
- Minimum 3 utilisateurs non-techniques
- Sc√©narios : Import taxonomie, import spatial, import mixte
- M√©triques : Time to completion, errors, satisfaction

**Crit√®res de succ√®s :**
- Import complet < 10 minutes (upload ‚Üí donn√©es import√©es)
- √âditions configuration < 3 en moyenne
- Satisfaction utilisateur > 4/5

---

## 8. Points de D√©cision Critiques

### 8.1 Naming Strategy

**Question :** Comment nommer les tables g√©n√©riques?

**Options :**

A. **Suffixe "_ref" syst√©matique** (Recommand√©)
```yaml
entities:
  species: {...}  # ‚Üí Table: species_ref
  sites: {...}    # ‚Üí Table: sites_ref
```

‚úÖ **Pros :**
- Convention claire et coh√©rente
- Facilite la distinction reference vs. data tables
- Transform configs predictibles

‚ùå **Cons :**
- Convention impos√©e

**B. User-defined table names**
```yaml
entities:
  species:
    table_name: custom_species_table  # Override optionnel
```

‚úÖ **Pros :** Flexibilit√© totale
‚ùå **Cons :** Peut cr√©er confusion, noms incoh√©rents

**Recommandation :** **Option A** (suffixe syst√©matique) avec possibilit√© d'override si absolument n√©cessaire

---

### 8.2 Hierarchy System

**Question :** Quel syst√®me de hi√©rarchie g√©n√©raliser?

**Options :**

A. **Nested Set uniquement** (Actuel)
- ‚úÖ Performance excellente pour requ√™tes de sous-arbres
- ‚ùå Insertions co√ªteuses
- ‚ùå Complexe √† maintenir

B. **Adjacency List** (Simple parent_id)
- ‚úÖ Simple, insertions rapides
- ‚ùå Requ√™tes de sous-arbres complexes (recursive CTEs)

C. **Hybrid** (Nested Set + Adjacency) (Recommand√©)
- ‚úÖ Meilleur des deux mondes
- ‚úÖ Fallback si nested set √©choue
- ‚ùå L√©g√®rement plus de storage

**Recommandation :** **Option C** - Hybrid

```yaml
entities:
  species:
    hierarchy:
      type: nested_set  # ou 'adjacency_list'
      levels: [family, genus, species]
```


---

## 9. M√©triques de Succ√®s

### 9.1 M√©triques Techniques

| M√©trique | Target | Mesure |
|----------|--------|--------|
| **Test Coverage** | > 85% | Lignes de code couvertes |
| **Performance Import** | < 10% d√©gradation | Temps d'import dataset test |
| **Auto-detection Accuracy** | > 85% | % configs correctes sans √©dition |
| **API Response Time** | < 2s | Temps analyse fichier |

### 9.2 M√©triques Utilisateur

| M√©trique | Target | Mesure |
|----------|--------|--------|
| **Time to First Import** | < 10 min | Temps upload ‚Üí import complet |
| **User Edits Required** | < 3 | Nb modifications config auto-g√©n√©r√©e |
| **Error Rate** | < 5% | % imports √©chouant |
| **User Satisfaction** | > 4/5 | Survey post-utilisation |

### 9.3 M√©triques Business

| M√©trique | Target | Mesure |
|----------|--------|--------|
| **Adoption Rate** | > 90% en 3 mois | % utilisateurs utilisant nouveau syst√®me |
| **New Entity Types** | > 5 types cr√©√©s | Diversit√© entit√©s custom cr√©√©es par utilisateurs |
| **Support Tickets** | < 2/semaine | Tickets li√©s √† import |
| **Documentation Quality** | > 4/5 | Feedback utilisateurs sur docs |
| **Bootstrap Success Rate** | > 80% | % bootstraps r√©ussis sans intervention |

---

## Conclusion & Next Steps

### R√©sum√©

Cette analyse ultrathink propose un refactoring direct du syst√®me d'import Niamoto en **3 phases sur 8 semaines** :

1. **Phase 1 (3 sem)** : Core abstractions + Generic import engine
2. **Phase 2 (3 sem)** : Bootstrap & auto-detection
3. **Phase 3 (2 sem)** : GUI integration

**Approche cl√© :** Refactoring direct sans contrainte de r√©trocompatibilit√©, architecture propre d√®s le d√©part.

**Avantages du statut alpha :**
- ‚úÖ Pas de code legacy √† maintenir
- ‚úÖ Architecture optimale sans compromis
- ‚úÖ Timeline r√©duite (8 vs 12 semaines)
- ‚úÖ Simplicit√© accrue

### Actions Imm√©diates

1. **Validation √âquipe** (Semaine 0)
   - Review ce document
   - D√©cisions sur points critiques (Naming, Hierarchy)
   - Validation timeline 8 semaines
   - Pr√©paration datasets de test

- ‚úÖ Module `src/niamoto/core/imports/config_models.py` introduit (Pydantic) pour mod√©liser les entit√©s/connexions d√©crites dans cette roadmap.
- ‚úÖ Tests unitaires `tests/core/imports/test_config_models.py` validant parsing et r√®gles de base (alias id/id_field, hi√©rarchie, options dataset).
- ‚úÖ ADR 0001 ¬´¬†Adoption de DuckDB¬†¬ª et ADR 0002 ¬´¬†Retrait des importeurs legacy¬†¬ª publi√©s (`docs/09-architecture/adr/`).
- ‚úÖ Entity Registry initiale (`src/niamoto/core/imports/registry.py`) + tests (`tests/core/imports/test_registry.py`).
- ‚úÖ Premi√®re abstraction `Database` hybride (DuckDB/SQLite) avec chargement d‚Äôextensions spatiales et API `fetch_all` partag√©e.
- ‚úÖ `TransformerService` et `ExporterService` instancient la registry et r√©solvent les tables via alias (fallback si absence).
- ‚úÖ Endpoints GUI (`/table-fields` & `/status`), loader `direct_reference` et transformer `geospatial_extractor` utilisent d√©sormais la registry + helpers DB, sans PRAGMA SQLite.
- üîú Prototype adaptateur DuckDB (connexion + introspection) et migration des services vers la registry.

2. **Setup Projet** (Semaine 0)
   - Cr√©er feature branch `feat/generic-import`
   - Setup environnement de d√©veloppement
   - Pr√©parer suite de tests avec datasets vari√©s
   - CI/CD pour tests automatiques

3. **Kick-off Phase 1** (Semaine 1)
   - D√©marrer impl√©mentation BaseEntityImporter
   - Analyser code existant pour extraction logique
   - Documentation technique en parall√®le

### Risques Majeurs √† Monitorer

1. ‚ö†Ô∏è **Complexit√© technique** : Nested sets et g√©om√©tries n√©cessitent attention
2. ‚ö†Ô∏è **Performance** : Benchmarking continu vs. baseline
3. ‚ö†Ô∏è **User Experience** : GUI doit √™tre intuitive, user testing d√®s que possible
4. ‚ö†Ô∏è **Over-engineering** : Garder la simplicit√©, it√©rations courtes

### Success Factors

- ‚úÖ Tests exhaustifs √† chaque √©tape
- ‚úÖ Benchmarking performance syst√©matique
- ‚úÖ Documentation parall√®le au d√©veloppement
- ‚úÖ User feedback pr√©coce (Phase 2)
- ‚úÖ Architecture simple et claire
- ‚úÖ Code reviews r√©guli√®res

### Livrables Finaux (Semaine 8)

- ‚úÖ Syst√®me d'import g√©n√©rique fonctionnel
- ‚úÖ Support entit√©s custom (hierarchical, spatial, categorical)
- ‚úÖ Bootstrap automatique avec > 85% accuracy
- ‚úÖ GUI compl√®te avec wizard d'import
- ‚úÖ Documentation compl√®te (technique + utilisateur)
- ‚úÖ Suite de tests > 85% coverage
- ‚úÖ Performance valid√©e

---

**Pr√™t √† d√©marrer?** üöÄ
## Annexe : Impact du Passage √† DuckDB sur la Refactorisation {#annexe-impact-duckdb}

### Contexte

DuckDB offre des capacit√©s significativement sup√©rieures √† SQLite pour l'import et l'analyse de donn√©es. Cette annexe analyse l'impact potentiel sur la refactorisation du syst√®me d'import.

---

## 1. Avantages Majeurs de DuckDB

### 1.1 Import Natif de Donn√©es

**Capacit√©s natives :**
```sql
-- SQLite : N√©cessite pandas + transformation
df = pd.read_csv('data.csv')
df.to_sql('table', conn)

-- DuckDB : Direct, avec d√©tection automatique
CREATE TABLE species AS
SELECT * FROM read_csv_auto('data.csv');

-- Avec options
CREATE TABLE species AS
SELECT * FROM read_csv('data.csv',
    auto_detect=true,
    header=true,
    delimiter=',',
    compression='gzip'
);
```

**Formats support√©s nativement :**
- CSV/TSV (avec `read_csv_auto()`)
- Parquet (`read_parquet()`)
- JSON (`read_json_auto()`)
- Excel via extension
- **GeoParquet** pour donn√©es spatiales

**Avantage :** √âlimine le besoin de pandas pour la plupart des imports.

### 1.2 D√©tection Automatique de Sch√©ma

```sql
-- DuckDB d√©tecte automatiquement les types
DESCRIBE SELECT * FROM read_csv_auto('observations.csv');

-- Output:
-- column_name | column_type | null | key | default | extra
-- id          | INTEGER     | NO   |     |         |
-- species_id  | VARCHAR     | YES  |     |         |
-- dbh         | DOUBLE      | YES  |     |         |
-- date        | DATE        | YES  |     |         |
```

**Impact sur notre refactorisation :**
- ‚úÖ `DataTypeDetector` peut devenir **beaucoup plus simple**
- ‚úÖ Utiliser directement les capacit√©s DuckDB au lieu de r√©inventer
- ‚úÖ R√©duction significative du code custom

### 1.3 Hi√©rarchies avec Recursive CTEs

**Actuellement (Nested Sets) :**
```sql
-- Complexe : calcul lft/rght, updates co√ªteux
UPDATE taxon_ref SET lft = ..., rght = ... -- Algorithme complexe
```

**Avec DuckDB (Recursive CTEs) :**
```sql
-- Simple et √©l√©gant
WITH RECURSIVE taxon_tree AS (
    -- Anchor: root nodes
    SELECT id, name, parent_id, 1 as level,
           CAST(name AS VARCHAR) as path
    FROM taxon_ref
    WHERE parent_id IS NULL

    UNION ALL

    -- Recursive: children
    SELECT t.id, t.name, t.parent_id, tt.level + 1,
           tt.path || ' > ' || t.name
    FROM taxon_ref t
    JOIN taxon_tree tt ON t.parent_id = tt.id
)
SELECT * FROM taxon_tree;

-- Obtenir tous les descendants d'un taxon
WITH RECURSIVE descendants AS (
    SELECT * FROM taxon_ref WHERE id = 42
    UNION ALL
    SELECT t.* FROM taxon_ref t
    JOIN descendants d ON t.parent_id = d.id
)
SELECT * FROM descendants;
```

**Avantages :**
- ‚úÖ **Beaucoup plus simple** que nested sets
- ‚úÖ Insertions/updates simples (juste parent_id)
- ‚úÖ Pas de recalcul lft/rght
- ‚úÖ Performance excellente pour queries hi√©rarchiques
- ‚úÖ Code `HierarchyManager` drastiquement simplifi√©

**Inconv√©nient nested sets (peut √™tre √©limin√©) :**
- ‚ùå Complexit√© du code
- ‚ùå Maintenance difficile
- ‚ùå Updates co√ªteux

### 1.4 Extension Spatial Native

```sql
-- Installation extension spatial
INSTALL spatial;
LOAD spatial;

-- Import GeoJSON direct
CREATE TABLE sites AS
SELECT * FROM ST_Read('sites.geojson');

-- Op√©rations spatiales
SELECT
    name,
    ST_AsText(geometry) as wkt,
    ST_Area(geometry) as area,
    ST_Centroid(geometry) as centroid
FROM sites;

-- Spatial joins
SELECT o.*, s.name as site_name
FROM observations o
JOIN sites s
ON ST_Contains(s.geometry, ST_Point(o.longitude, o.latitude));
```

**Avantages :**
- ‚úÖ Support natif WKT, WKB, GeoJSON
- ‚úÖ Op√©rations spatiales performantes
- ‚úÖ Pas besoin de shapely/geopandas pour validation
- ‚úÖ `GeometryValidator` simplifi√©

### 1.5 Performance Analytique

**Optimisations OLAP :**
- Compression automatique (r√©duction 5-10x taille)
- Vectorisation (SIMD)
- Parall√©lisme automatique
- Cache-aware algorithms

**Impact transform phase :**
```sql
-- Agr√©gations complexes beaucoup plus rapides
SELECT
    taxon_id,
    COUNT(*) as occurrences,
    AVG(dbh) as avg_dbh,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY height) as median_height,
    ST_ConvexHull(ST_Collect(geometry)) as distribution_hull
FROM observations
GROUP BY taxon_id;
```

Performance : **10-100x plus rapide** que SQLite sur agr√©gations complexes.

---

## 2. Impact sur l'Architecture de la Refactorisation

### 2.1 Simplifications Majeures

**Composants qui deviennent plus simples :**

| Composant | Actuellement | Avec DuckDB | R√©duction Complexit√© |
|-----------|--------------|-------------|---------------------|
| `DataTypeDetector` | ~300 lignes analyse pandas | ~50 lignes wrapper DuckDB | -83% |
| `HierarchyManager` | ~500 lignes nested sets | ~100 lignes recursive CTEs | -80% |
| `GeometryValidator` | ~200 lignes shapely | ~50 lignes spatial extension | -75% |
| `DynamicTableFactory` | ~400 lignes SQLAlchemy | ~150 lignes SQL direct | -62% |
| **Total** | **~1400 lignes** | **~350 lignes** | **-75%** |

### 2.2 Nouvelle Architecture Simplifi√©e

```python
# Avec DuckDB, l'import devient trivial
class DuckDBEntityImporter:
    """Simplified importer leveraging DuckDB capabilities"""

    def import_entity(self, entity_name: str, config: Dict) -> int:
        """Import entity using DuckDB native functions"""
        source = config['source']

        # 1. Auto-detect and create table (ONE LINE!)
        self.db.execute(f"""
            CREATE TABLE {entity_name}_ref AS
            SELECT * FROM read_csv_auto('{source}')
        """)

        # 2. Add hierarchy columns if needed (simple)
        if config.get('kind') == 'hierarchical':
            self._add_hierarchy_columns(entity_name)

        # 3. Validate spatial data if needed
        if config.get('kind') == 'spatial':
            self._validate_spatial(entity_name, config['geometry']['field'])

        return self.db.execute(f"SELECT COUNT(*) FROM {entity_name}_ref").fetchone()[0]

    def _add_hierarchy_columns(self, table: str):
        """Just add parent_id - no lft/rght needed!"""
        # DuckDB recursive CTEs handle hierarchy traversal
        pass
```

**Code r√©duit de ~75%** car on utilise les capacit√©s natives de DuckDB!

### 2.3 Bootstrap Simplifi√©

```python
class DuckDBDataProfiler:
    """Leverage DuckDB's schema detection"""

    def profile(self, file_path: Path) -> DatasetProfile:
        # DuckDB fait le travail dur
        schema = self.db.execute(f"""
            DESCRIBE SELECT * FROM read_csv_auto('{file_path}')
        """).fetchdf()

        # Juste enrichir avec d√©tection s√©mantique
        columns = [
            self._enrich_column_profile(row)
            for _, row in schema.iterrows()
        ]

        return DatasetProfile(
            file_path=file_path,
            columns=columns,
            detected_type=self._detect_type(columns)
        )
```

---

## 3. Consid√©rations et Trade-offs

### 3.1 Avantages DuckDB pour Niamoto

‚úÖ **Import simplifi√©** : 75% moins de code
‚úÖ **Performance** : 10-100x plus rapide sur agr√©gations
‚úÖ **Hi√©rarchies simples** : Recursive CTEs au lieu de nested sets
‚úÖ **Spatial natif** : Meilleur support g√©ospatial
‚úÖ **Sch√©ma dynamique** : D√©tection automatique
‚úÖ **Files-as-tables** : Peut query directement des CSV sans import
‚úÖ **Compression** : Bases de donn√©es 5-10x plus petites

### 3.2 Inconv√©nients / Consid√©rations

‚ö†Ô∏è **Adoption** : DuckDB moins connu que SQLite
‚ö†Ô∏è **API diff√©rente** : N√©cessite migration du code existant
‚ö†Ô∏è **Extensions** : Spatial extension n√©cessite installation
‚ö†Ô∏è **Write workload** : SQLite parfois meilleur pour write-heavy (mais Niamoto est read-heavy apr√®s import)

### 3.3 Compatibilit√© avec Architecture Actuelle

**Migration SQLite ‚Üí DuckDB :**
```python
# DuckDB peut lire directement des bases SQLite!
import duckdb

conn = duckdb.connect('niamoto.duckdb')

# Importer depuis SQLite
conn.execute("INSTALL sqlite")
conn.execute("LOAD sqlite")
conn.execute("ATTACH 'old_niamoto.db' AS sqlite_db (TYPE sqlite)")

# Copier tables
conn.execute("CREATE TABLE taxon_ref AS SELECT * FROM sqlite_db.taxon_ref")
```

---

## 4. Recommandations

### 4.1 Sc√©nario Optimal : Refactorisation + DuckDB

**Timeline ajust√©e :**

| Phase | Avec SQLite | Avec DuckDB | √âconomie |
|-------|-------------|-------------|----------|
| Phase 1 | 3 semaines | **2 semaines** | -33% |
| Phase 2 | 3 semaines | **2 semaines** | -33% |
| Phase 3 | 2 semaines | 2 semaines | 0% |
| **Total** | **8 semaines** | **6 semaines** | **-25%** |

**Justification r√©duction Phase 1 :**
- DataTypeDetector : Utilise `read_csv_auto()` ‚Üí -2 jours
- HierarchyManager : Recursive CTEs simples ‚Üí -2 jours
- GeometryValidator : Extension spatial ‚Üí -1 jour
- DynamicTableFactory : SQL direct ‚Üí -2 jours

**Justification r√©duction Phase 2 :**
- DataProfiler : Wrapper DuckDB ‚Üí -3 jours
- ConfigGenerator : Moins de validation n√©cessaire ‚Üí -2 jours

### 4.2 Proposition Concr√®te

**Option A : Migration imm√©diate √† DuckDB** ‚úÖ **RECOMMAND√â**

**Avantages :**
- Architecture plus simple d√®s le d√©part
- Timeline r√©duite (6 vs 8 semaines)
- Code plus maintenable (75% moins de lignes)
- Performance sup√©rieure
- Fonctionnalit√©s natives (spatial, recursive CTEs)

**Risques :**
- N√©cessite apprentissage DuckDB
- Migration bases existantes (mais facile avec ATTACH)

**Option B : SQLite d'abord, DuckDB plus tard**

**Avantages :**
- Pas de changement de technologie pendant refactorisation
- Familiarit√© avec SQLite

**Inconv√©nients :**
- Code plus complexe
- Timeline plus longue
- Refactorisation suppl√©mentaire n√©cessaire pour DuckDB plus tard

### 4.3 Architecture Cible avec DuckDB

```
src/niamoto/core/imports/
‚îú‚îÄ‚îÄ base.py                    # BaseEntityImporter (simplifi√©)
‚îú‚îÄ‚îÄ duckdb_importer.py        # DuckDB-specific importer (NEW)
‚îú‚îÄ‚îÄ hierarchy.py              # Recursive CTEs (simple)
‚îú‚îÄ‚îÄ spatial.py                # Wrapper spatial extension
‚îú‚îÄ‚îÄ profiler.py               # Utilise DuckDB schema detection
‚îú‚îÄ‚îÄ config_generator.py       # Simplifi√©
‚îî‚îÄ‚îÄ bootstrap.py              # Orchestration

# Hi√©rarchies
- Pas de nested sets (lft/rght)
+ Adjacency list (parent_id)
+ Recursive CTEs pour queries

# Spatial
- Pas de WKT custom validation
+ Extension spatial native
+ ST_* functions

# Import
- Pas de pandas transformation
+ read_csv_auto() direct
+ CREATE TABLE AS SELECT
```

---

## 5. D√©cision Requise

### Question Cl√©

**Devons-nous int√©grer le passage √† DuckDB dans cette refactorisation?**

**Arguments POUR (‚≠ê Recommand√©) :**
- ‚úÖ Simplifie drastiquement la refactorisation
- ‚úÖ R√©duit timeline de 25% (6 vs 8 semaines)
- ‚úÖ Code 75% plus simple
- ‚úÖ Performance sup√©rieure
- ‚úÖ Pas de refactorisation future n√©cessaire
- ‚úÖ Niamoto est id√©al pour DuckDB (read-heavy, analytique)

**Arguments CONTRE :**
- ‚ö†Ô∏è Changement technologique suppl√©mentaire
- ‚ö†Ô∏è Courbe d'apprentissage
- ‚ö†Ô∏è Migration bases existantes (mais simple)

### Recommandation Finale

**JE RECOMMANDE FORTEMENT d'int√©grer DuckDB dans cette refactorisation.**

**Justification :**
1. Architecture devient **beaucoup plus simple**
2. Timeline **r√©duite** au lieu d'allong√©e
3. √âvite une **double refactorisation** (SQLite ‚Üí DuckDB plus tard)
4. DuckDB est **parfaitement adapt√©** √† Niamoto (OLAP, analytique)
5. Migration facile gr√¢ce √† `ATTACH` SQLite

**Timeline ajust√©e : 6 semaines au lieu de 8** üöÄ

---

## Conclusion

Le passage √† DuckDB transforme cette refactorisation d'un projet complexe en un projet **significativement plus simple**. Les capacit√©s natives de DuckDB (import automatique, recursive CTEs, spatial extension) √©liminent la majorit√© de la complexit√© du code custom.

**Next step :** D√©cision sur DuckDB + ajustement planning si accept√©.
