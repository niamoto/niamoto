# Guide d'utilisation et modification du d√©tecteur ML

## üß™ Comment tester le d√©tecteur

### 1. Test rapide avec le mod√®le existant

```bash
# Depuis le r√©pertoire Niamoto
cd /Users/julienbarbe/Dev/Niamoto/Niamoto

# Test direct du d√©tecteur
uv run python -c "
import pandas as pd
import numpy as np
from src.niamoto.core.imports.ml_detector import MLColumnDetector

# Charger le mod√®le
detector = MLColumnDetector()
detector.load_model('models/column_detector.pkl')

# Tester sur des donn√©es
dbh_data = pd.Series([15.5, 23.2, 45.1, 67.3])  # Valeurs DBH-like
print('DBH test:', detector.predict(dbh_data))

species_data = pd.Series(['Araucaria columnaris', 'Agathis montana'])
print('Species test:', detector.predict(species_data))
"
```

### 2. Test avec des CSV r√©els

```bash
# Cr√©er un fichier test avec colonnes mal nomm√©es
cat > test_data.csv << EOF
X1,toto,machin,truc
15.5,Araucaria columnaris,12.3,0.65
23.2,Agathis montana,18.5,0.72
45.1,Podocarpus minor,25.2,0.58
EOF

# Tester avec le profiler
uv run python -c "
from pathlib import Path
from src.niamoto.core.imports.profiler import DataProfiler
from src.niamoto.core.imports.ml_detector import MLColumnDetector

# Cr√©er profiler avec ML
ml = MLColumnDetector()
ml.load_model('models/column_detector.pkl')
profiler = DataProfiler(ml_detector=ml)

# Analyser le fichier
profile = profiler.profile(Path('test_data.csv'))

# Afficher les r√©sultats
for col in profile.columns:
    print(f'{col.name}: {col.semantic_type} (confiance: {col.confidence:.0%})')
"
```

### 3. Test dans une instance Niamoto

```bash
cd test-instance/niamoto-og

# Tester sur de vraies donn√©es
uv run python -c "
import pandas as pd
from pathlib import Path
import sys
sys.path.insert(0, '../../src')

from niamoto.core.imports.ml_detector import MLColumnDetector

# Charger vos donn√©es
df = pd.read_csv('imports/occurrences.csv')

# Tester le d√©tecteur
detector = MLColumnDetector()
detector.load_model('../../models/column_detector.pkl')

for col in df.columns[:5]:  # Tester les 5 premi√®res colonnes
    pred_type, confidence = detector.predict(df[col])
    print(f'{col}: {pred_type} ({confidence:.0%})')
"
```

### 4. Lancer les tests unitaires

```bash
# Tests du d√©tecteur ML
uv run pytest tests/core/imports/test_ml_detector.py -v

# Tests d'int√©gration avec le profiler
uv run pytest tests/core/imports/test_profiler_ml.py -v

# Tous les tests imports
uv run pytest tests/core/imports/ -v
```

## üîß Comment modifier le d√©tecteur

### 1. Ajouter un nouveau type de colonne

**√âtape 1: Modifier la configuration** (`src/niamoto/core/imports/ml_detector.py`)

```python
@dataclass
class ColumnTypeConfig:
    TYPES = [
        'diameter',
        'height',
        # Ajouter votre nouveau type ici
        'basal_area',      # <-- NOUVEAU
        'crown_diameter',  # <-- NOUVEAU
        # ...
    ]
```

**√âtape 2: Ajouter des donn√©es d'entra√Ænement** (`scripts/train_column_detector.py`)

```python
def generate_synthetic_training_data():
    # ... code existant ...

    # Ajouter des exemples pour basal_area
    logger.info("Generating basal area examples...")
    for i in range(30):
        size = np.random.randint(50, 300)
        # Basal area en m¬≤ (g√©n√©ralement 0.001 √† 10)
        basal_area = np.random.gamma(shape=2, scale=0.5, size=size)
        basal_area = np.clip(basal_area, 0.001, 10)
        training_data.append((pd.Series(basal_area), 'basal_area'))
```

**√âtape 3: Mapper vers type s√©mantique** (`src/niamoto/core/imports/profiler.py`)

```python
ml_to_semantic_map = {
    # ... mappings existants ...
    'basal_area': 'measurement.basal_area',
    'crown_diameter': 'measurement.crown_diameter',
}
```

**√âtape 4: R√©-entra√Æner le mod√®le**

```bash
uv run python scripts/train_column_detector.py
```

### 2. Am√©liorer la d√©tection d'un type existant

**Option A: Ajouter plus de features**

```python
def _extract_numeric_features(self, series: pd.Series) -> List[float]:
    features = []

    # Features existantes...

    # NOUVELLES features pour am√©liorer la d√©tection
    # Ex: coefficient de variation pour mieux d√©tecter DBH
    cv = series.std() / series.mean() if series.mean() > 0 else 0
    features.append(cv)

    # Asym√©trie log-normale (typique pour DBH)
    log_skew = np.log(series[series > 0]).skew() if (series > 0).any() else 0
    features.append(log_skew)

    # N'oubliez pas d'ajuster N_FEATURES dans ColumnTypeConfig !
```

**Option B: Am√©liorer les r√®gles de fallback**

```python
def _rule_based_detection(self, series: pd.Series, return_all: bool = False):
    if pd.api.types.is_numeric_dtype(series):
        clean = series.dropna()

        # Am√©liorer la d√©tection DBH
        if 5 < clean.mean() < 100 and clean.max() < 500:
            # V√©rifier aussi la distribution log-normale
            log_data = np.log(clean[clean > 0])
            if abs(log_data.skew()) < 0.5:  # Log-normal typique
                result = ('diameter', 0.85)  # Confiance augment√©e
```

### 3. Utiliser des donn√©es r√©elles pour l'entra√Ænement

```python
# Modifier scripts/train_column_detector.py

def load_real_data_if_available():
    """Charger vos propres donn√©es annot√©es"""

    real_data = []

    # Charger un fichier avec colonnes connues
    your_file = Path("data/annotated_columns.csv")
    if your_file.exists():
        df = pd.read_csv(your_file)

        # Mapper les colonnes que vous connaissez
        column_mappings = {
            'dbh_cm': 'diameter',
            'hauteur_m': 'height',
            'nom_espece': 'species_name',
            'surface_foliaire': 'leaf_area',
            # ... vos colonnes
        }

        for col_name, col_type in column_mappings.items():
            if col_name in df.columns:
                real_data.append((df[col_name], col_type))

    return real_data
```

### 4. Ajuster les hyperparam√®tres du mod√®le

```python
# Dans ml_detector.py, ligne 76
self.model = RandomForestClassifier(
    n_estimators=200,      # Augmenter pour plus de pr√©cision (√©tait 100)
    max_depth=15,          # Augmenter pour capturer plus de complexit√© (√©tait 10)
    min_samples_split=3,   # R√©duire pour plus de sensibilit√© (√©tait 5)
    min_samples_leaf=1,    # R√©duire pour plus de d√©tail (√©tait 2)
    random_state=42,
    n_jobs=-1
)
```

### 5. Tester vos modifications

```python
# Script de test personnalis√©
cat > test_my_changes.py << 'EOF'
#!/usr/bin/env python
"""Tester mes modifications du d√©tecteur"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys
sys.path.insert(0, 'src')

from niamoto.core.imports.ml_detector import MLColumnDetector

# 1. Cr√©er des donn√©es de test pour votre nouveau type
test_data = {
    'basal_area': pd.Series(np.random.gamma(2, 0.5, 100)),
    'crown_diameter': pd.Series(np.random.normal(5, 2, 100)),
}

# 2. Entra√Æner un nouveau mod√®le
detector = MLColumnDetector()
training_data = []

for col_type, data in test_data.items():
    # Cr√©er plusieurs exemples
    for _ in range(10):
        sample = data.sample(50)
        training_data.append((sample, col_type))

# Ajouter des types existants
training_data.append((pd.Series(np.random.lognormal(3, 0.8, 50)), 'diameter'))

detector.train(training_data)

# 3. Tester la d√©tection
print("Test de d√©tection:")
for col_type, data in test_data.items():
    pred, conf = detector.predict(data)
    print(f"  {col_type}: d√©tect√© comme {pred} ({conf:.0%})")

# 4. Sauvegarder si satisfait
detector.save_model(Path('models/column_detector_custom.pkl'))
EOF

uv run python test_my_changes.py
```

## üìä Analyser les performances

```python
# Script pour analyser les erreurs
cat > analyze_errors.py << 'EOF'
import pandas as pd
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Charger vos donn√©es de test
# ...

# Cr√©er matrice de confusion
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True)
plt.show()

# Identifier les cas probl√©matiques
errors = [(true, pred, data) for true, pred, data in zip(y_true, y_pred, X_test) if true != pred]
print(f"Erreurs: {len(errors)}/{len(y_true)}")
for true, pred, sample in errors[:5]:
    print(f"  Attendu: {true}, Pr√©dit: {pred}")
    print(f"  Features: {sample[:5]}...")  # Premi√®res features
EOF
```

## üöÄ Workflow complet de modification

1. **Backup du mod√®le existant**
   ```bash
   cp models/column_detector.pkl models/column_detector_backup.pkl
   ```

2. **Modifier le code** (voir sections ci-dessus)

3. **R√©-entra√Æner**
   ```bash
   uv run python scripts/train_column_detector.py
   ```

4. **Tester**
   ```bash
   uv run pytest tests/core/imports/test_ml_detector.py -v
   ```

5. **Valider sur donn√©es r√©elles**
   ```bash
   # Tester sur vos vraies donn√©es
   uv run python test_my_changes.py
   ```

6. **Commit si satisfait**
   ```bash
   git add -A
   git commit -m "feat: am√©lioration d√©tection ML pour [votre changement]"
   ```

## üí° Tips

- **Logs**: Activer les logs pour debug
  ```python
  import logging
  logging.basicConfig(level=logging.DEBUG)
  ```

- **Visualiser les features**:
  ```python
  features = detector.extract_features(your_series)
  print("Features:", features)
  print("Feature names:", detector._get_feature_names())
  ```

- **Tester sans r√©-entra√Æner**:
  ```python
  # Utiliser le fallback rule-based
  detector = MLColumnDetector()  # Sans charger de mod√®le
  result = detector.predict(your_data)  # Utilisera les r√®gles
  ```

- **Comparer avant/apr√®s**:
  ```python
  old_detector = MLColumnDetector()
  old_detector.load_model('models/column_detector_backup.pkl')

  new_detector = MLColumnDetector()
  new_detector.load_model('models/column_detector.pkl')

  # Comparer sur m√™mes donn√©es
  old_pred = old_detector.predict(test_data)
  new_pred = new_detector.predict(test_data)
  print(f"Ancien: {old_pred}, Nouveau: {new_pred}")
  ```
